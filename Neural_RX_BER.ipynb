{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7bcb26b8-3f1e-45e8-b2f1-fef291f27c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available : 0\n",
      "Tue Aug  5 17:23:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:AC:00.0 Off |                  Off |\n",
      "| 30%   48C    P8             33W /  300W |      34MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:CA:00.0 Off |                  Off |\n",
      "| 30%   49C    P8             31W /  300W |   18485MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2763      G   /usr/lib/xorg/Xorg                             10MiB |\n",
      "|    0   N/A  N/A      2905      G   /usr/bin/gnome-shell                            8MiB |\n",
      "|    1   N/A  N/A      2763      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   3040970      C   ...iniforge3/envs/aihub/bin/python3.10      18464MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "if gpus:\n",
    "    gpu_num = 0 # Number of the GPU to be used\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n",
    "        print('Only GPU number', gpu_num, 'used.')\n",
    "        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "!nvidia-smi\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "from sionna.phy.mimo import StreamManagement\n",
    "\n",
    "#from sionna.phy.ofdm import CSIGridMapper\n",
    "from sionna.phy.ofdm import ResourceGrid, ResourceGridMapper, ResourceGridDemapper, LSChannelEstimator, LMMSEEqualizer\n",
    "from sionna.phy.ofdm import OFDMModulator, OFDMDemodulator, RemoveNulledSubcarriers, ZFEqualizer\n",
    "\n",
    "from sionna.phy.channel.tr38901 import Antenna, AntennaArray, CDL, UMi, UMa, RMa\n",
    "from sionna.phy.channel import gen_single_sector_topology as gen_topology\n",
    "from sionna.phy.channel import subcarrier_frequencies, cir_to_ofdm_channel, cir_to_time_channel\n",
    "from sionna.phy.channel import ApplyOFDMChannel, ApplyTimeChannel, OFDMChannel\n",
    "\n",
    "from sionna.phy.fec.ldpc.encoding import LDPC5GEncoder\n",
    "from sionna.phy.fec.ldpc.decoding import LDPC5GDecoder\n",
    "\n",
    "from sionna.phy.mapping import Mapper, Demapper, BinarySource, QAMSource\n",
    "\n",
    "from sionna.phy.utils import ebnodb2no, sim_ber\n",
    "from sionna.phy.utils.metrics import compute_ber\n",
    "# from sionna.ofdm import CSIGridMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f502c95c-8050-4447-8b53-08fe99f0ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx_Power: 0.1772876696043162\n",
      "Noise: -120.22878745280337\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = \"umi\"\n",
    "carrier_frequency = 5.3e9\n",
    "direction = \"uplink\"\n",
    "num_ut = 1\n",
    "batch_size = 5000\n",
    "\n",
    "num_RB   = 8\n",
    "num_symbols = 14\n",
    "num_SC = 96\n",
    "fft_size = 12*num_RB\n",
    "subcarrier_spacing = 30e3\n",
    "\n",
    "## Tx Power\n",
    "TxPower_dBm     = 20 # in dBm\n",
    "TxPower         = 10**((TxPower_dBm-30)/10)\n",
    "TxPower_SC      = TxPower/(num_SC)\n",
    "TxPower_SC_dBm  = 10*np.log10(TxPower_SC*1000)\n",
    "\n",
    "# Noise Power\n",
    "noise_psd       = -174                                  # in dBm/Hz\n",
    "noise_figure    = 9                                     # in dB\n",
    "noise_SC_Watt   = 10**((noise_psd + noise_figure - 30)/10)*subcarrier_spacing\n",
    "noise_SC_dBm    = 10*np.log10(noise_SC_Watt*1000)\n",
    "\n",
    "print(\"Tx_Power:\",TxPower_SC_dBm)\n",
    "print(\"Noise:\",noise_SC_dBm)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "# Define the UT antenna array\n",
    "ut_array = Antenna(polarization=\"single\",\n",
    "                   polarization_type=\"V\",\n",
    "                   antenna_pattern=\"omni\",\n",
    "                   carrier_frequency=carrier_frequency)\n",
    "\n",
    "# Define the BS antenna array\n",
    "bs_array = AntennaArray(num_rows=1,\n",
    "                             num_cols=1, # We want to transmitter to be equiped with the 16 rx antennas\n",
    "                             vertical_spacing=0.5,\n",
    "                             horizontal_spacing=0.5,\n",
    "                             polarization_type=\"V\",\n",
    "                             antenna_pattern=\"omni\",\n",
    "                             polarization=\"single\",\n",
    "                             carrier_frequency=5.3e9)\n",
    "\n",
    "# Create channel model\n",
    "channel_model = UMi(carrier_frequency=carrier_frequency,\n",
    "                    o2i_model=\"low\",\n",
    "                    ut_array=ut_array,\n",
    "                    bs_array=bs_array,\n",
    "                    direction=direction,\n",
    "                    enable_pathloss=True,\n",
    "                    enable_shadow_fading=True)\n",
    "\n",
    "# Generate the topology\n",
    "ut_dist = 10\n",
    "topology = gen_topology(batch_size, num_ut, scenario,min_bs_ut_dist=ut_dist, min_ut_velocity=0.33, max_ut_velocity=8.33,indoor_probability=0)\n",
    "\n",
    "# Set the topology\n",
    "channel_model.set_topology(*topology)\n",
    "ut_loc = topology[0]\n",
    "bs_loc = topology[1]\n",
    "ut_vel = topology[4]\n",
    "\n",
    "# The number of transmitted streams is equal to the number of UT antennas\n",
    "num_streams_per_tx = 1\n",
    "\n",
    "# Create an RX-TX association matrix\n",
    "# rx_tx_association[i,j]=1 means that receiver i gets at least one stream\n",
    "# from transmitter j. Depending on the transmission direction (uplink or downlink),\n",
    "# the role of UT and BS can change. However, as we have only a single\n",
    "# transmitter and receiver, this does not matter:\n",
    "rx_tx_association = np.zeros([1, num_ut])\n",
    "rx_tx_association[:, 0] = 1\n",
    "#rx_tx_association[:, 1] = 1\n",
    "\n",
    "# Instantiate a StreamManagement object\n",
    "# This determines which data streams are determined for which receiver.\n",
    "# In this simple setup, this is fairly simple. However, it can get complicated\n",
    "# for simulations with many transmitters and receivers.\n",
    "sm = StreamManagement(rx_tx_association, num_streams_per_tx)\n",
    "\n",
    "rg = ResourceGrid(num_ofdm_symbols=14,\n",
    "                  fft_size=fft_size,\n",
    "                  subcarrier_spacing=subcarrier_spacing,\n",
    "                  num_tx=num_ut,\n",
    "                  num_streams_per_tx=num_streams_per_tx,\n",
    "                  cyclic_prefix_length=0,\n",
    "                  pilot_pattern = \"kronecker\",\n",
    "                  pilot_ofdm_symbol_indices = [1])\n",
    "rg.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8812cf80-3d12-4118-97b2-adad62b608ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bits_per_symbol = 2 # QPSK modulation\n",
    "coderate = 0.3 # The code rate\n",
    "n = int(rg.num_data_symbols*num_bits_per_symbol)  # Number of coded bits\n",
    "k = int(n*coderate) # Number of information bits\n",
    "\n",
    "# The binary source will create batches of information bits\n",
    "binary_source = BinarySource()\n",
    "qam_source = QAMSource(num_bits_per_symbol)\n",
    "\n",
    "# The encoder maps information bits to coded bits\n",
    "#encoder = LDPC5GEncoder(k, n)\n",
    "\n",
    "# The mapper maps blocks of information bits to constellation symbols\n",
    "mapper = Mapper(\"qam\", num_bits_per_symbol)\n",
    "\n",
    "# The resource grid mapper maps symbols onto an OFDM resource grid\n",
    "rg_mapper = ResourceGridMapper(rg)\n",
    "\n",
    "# CSI-RS mapper\n",
    "#csi_mapper = CSIGridMapper(rg)\n",
    "\n",
    "#rg_demap = ResourceGridDemapper(rg, sm)\n",
    "\n",
    "# This function removes nulled subcarriers from any tensor having the shape of a resource grid\n",
    "remove_nulled_scs = RemoveNulledSubcarriers(rg)\n",
    "\n",
    "# CSI-RS estimator will provide the raw channel estimates\n",
    "#csi_est = CSIrsChannelEstimator(rg, sm)\n",
    "#csi_est = CSIrsChannelEstimator(rg, sm, TxPower_SC, interpolation_type = 'lin')\n",
    "\n",
    "# The LMMSE equalizer will provide soft symbols together with noise variance estimates\n",
    "zf_equ = ZFEqualizer(rg, sm)\n",
    "\n",
    "# The demapper produces LLR for all coded bits\n",
    "demapper = Demapper(\"app\", \"qam\", num_bits_per_symbol)\n",
    "\n",
    "# The decoder provides hard-decisions on the information bits\n",
    "#decoder = LDPC5GDecoder(encoder, hard_out=True)\n",
    "\n",
    "# OFDM CHannel\n",
    "ofdm_channel = OFDMChannel(channel_model, rg, add_awgn=True, normalize_channel=False, return_channel=True)\n",
    "channel_freq = ApplyOFDMChannel(add_awgn=True)\n",
    "frequencies = subcarrier_frequencies(rg.fft_size, rg.subcarrier_spacing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a7457-9bf5-4bfc-9062-c2655bcebc6a",
   "metadata": {},
   "source": [
    "### Classical Communication System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d49cb33a-8212-4234-9ae8-35774ccc7f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received Signal Shape: (5000, 1, 1, 14, 96)\n",
      "CSI Est NMSE : 0.0\n",
      "BER: 0.035915705128205126\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# --------- Transmitter ---------\n",
    "b = binary_source([batch_size, num_ut, rg.num_streams_per_tx, n])  # n must equal (#data_REs * bits_per_sym)\n",
    "# c = encoder(b)  # keep commented for uncoded link\n",
    "\n",
    "x    = mapper(b)                 # map bits -> constellation symbols\n",
    "x_rg = rg_mapper(x)              # map to OFDM resource grid (data REs only)\n",
    "x_rg = np.sqrt(TxPower_SC) * x_rg\n",
    "\n",
    "# --------- Channel ---------\n",
    "a, tau = channel_model(num_time_samples=rg.num_ofdm_symbols,\n",
    "                       sampling_frequency=1/rg.ofdm_symbol_duration)\n",
    "h_freq = cir_to_ofdm_channel(frequencies, a, tau, normalize=False)\n",
    "\n",
    "# --------- Receive ---------\n",
    "no = noise_SC_Watt\n",
    "y  = channel_freq(x_rg, h_freq, no)\n",
    "print(\"Received Signal Shape:\", y.shape)  # e.g., (N,1,1,S,F)\n",
    "\n",
    "# Channel estimation (here, perfect CSI for simplicity)\n",
    "h_freq_d_DL_hat = h_freq\n",
    "\n",
    "# ZF equalization\n",
    "x_hat, no_eff = zf_equ(y, h_freq_d_DL_hat, tf.zeros_like(h_freq_d_DL_hat), no)\n",
    "x_hat = x_hat / np.sqrt(TxPower_SC)\n",
    "\n",
    "# Demap to bit LLRs for **data REs only**\n",
    "llr = demapper(x_hat, no_eff)  # shape typically (N, 1, 1, E) or (N,1,1,S,F,B) depending on demapper\n",
    "\n",
    "# --------- Uncoded hard decisions (NO LDPC decoder here) ---------\n",
    "# Flatten both LLRs and labels, align lengths, and compute BER\n",
    "llr_flat = tf.reshape(llr, [tf.shape(llr)[0], -1])              # (N, E)\n",
    "b_hat    = tf.cast(llr_flat > 0.0, tf.float32)                  # (N, E)\n",
    "\n",
    "b_flat   = tf.reshape(tf.cast(b, tf.float32), [tf.shape(b)[0], -1])  # (N, n)\n",
    "\n",
    "# Align in case shapes differ by a few bits (shouldn't if n = #data_REs * B)\n",
    "T_pred = tf.shape(b_hat)[1]\n",
    "T_true = tf.shape(b_flat)[1]\n",
    "T_min  = tf.minimum(T_pred, T_true)\n",
    "b_hat  = b_hat[:, :T_min]\n",
    "b_flat = b_flat[:, :T_min]\n",
    "\n",
    "# --------- Metrics ---------\n",
    "mse = tf.reduce_mean(tf.abs(h_freq_d_DL_hat - h_freq)**2)\n",
    "nmse = mse / (tf.reduce_mean(tf.abs(h_freq)**2) + 1e-12)\n",
    "print(\"CSI Est NMSE :\", nmse.numpy())\n",
    "\n",
    "ber = compute_ber(b_flat, b_hat).numpy()\n",
    "print(\"BER:\", ber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f08610f2-a69a-41d0-bf4b-c0d3206e3791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rx Signal Shape : (5000, 1, 1, 14, 96)\n",
      "y_data  : (5000, 1, 14, 96) <dtype: 'complex64'>\n",
      "h_data  : (5000, 1, 14, 96) <dtype: 'complex64'>\n",
      "p_data  : (5000, 1, 14, 96) <dtype: 'complex64'>\n",
      "llr_probs: (5000, 1, 1, 2496) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# --- Inputs already defined earlier in your notebook ---\n",
    "# batch_size, num_ut, rg (with S=rg.num_ofdm_symbols, F=rg.fft_size-nulled), mapper, rg_mapper\n",
    "# TxPower_SC, channel_model, channel_freq, zf_equ, demapper, frequencies\n",
    "# binary_source, num_bits_per_symbol, etc.\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# Short-hands for dims\n",
    "# ----------------------------\n",
    "S = rg.num_ofdm_symbols  # 14\n",
    "F = num_SC               # 96  (active SC count used by your RG)\n",
    "B = batch_size\n",
    "\n",
    "# ----------------------------\n",
    "# Transmitter (data)\n",
    "# ----------------------------\n",
    "n  = int(rg.num_data_symbols * num_bits_per_symbol)\n",
    "b  = binary_source([B, num_ut, rg.num_streams_per_tx, n])\n",
    "\n",
    "x    = mapper(b)                       # (B, num_ut, num_streams, n_symbols_used_per_stream)\n",
    "x_rg = rg_mapper(x)                    # (B, 1, 1, S, F) complex\n",
    "x_rg = tf.cast(np.sqrt(TxPower_SC), tf.complex64) * x_rg\n",
    "\n",
    "# ----------------------------\n",
    "# Build pilots on symbol 1 only\n",
    "# ----------------------------\n",
    "# one-hot mask over OFDM symbols: 1 only at index=1\n",
    "sym1_mask = tf.one_hot(1, depth=S, dtype=tf.float32)      # (S,)\n",
    "sym1_mask = tf.reshape(sym1_mask, [1, 1, 1, S, 1])        # (1,1,1,S,1)\n",
    "\n",
    "# QPSK pilot value ( (1+j)/sqrt(2) ), force float32 -> complex64\n",
    "val32 = tf.constant(1/np.sqrt(2.0), dtype=tf.float32)\n",
    "re    = tf.fill([B, 1, 1, 1, F], val32)                   # (B,1,1,1,F)\n",
    "im    = tf.fill([B, 1, 1, 1, F], val32)                   # (B,1,1,1,F)\n",
    "pval  = tf.complex(re, im)                                 # (B,1,1,1,F) complex64\n",
    "\n",
    "# pilot grid, nonzero only on symbol 1\n",
    "p_rg = tf.cast(sym1_mask, tf.complex64) * pval            # (B,1,1,S,F) complex64\n",
    "\n",
    "# ----------------------------\n",
    "# Replace symbol 1 of data with pilots (no shape change)\n",
    "# ----------------------------\n",
    "one_f  = tf.cast(1.0, tf.float32)\n",
    "m_data = tf.cast(1.0 - sym1_mask, tf.complex64)           # zeros at sym1, ones elsewhere\n",
    "x_rg   = x_rg * m_data + p_rg                             # (B,1,1,S,F)\n",
    "\n",
    "# ----------------------------\n",
    "# Channel (frequency response & apply)\n",
    "# ----------------------------\n",
    "a, tau  = channel_model(num_time_samples=S, sampling_frequency=1/rg.ofdm_symbol_duration)\n",
    "h_freq  = cir_to_ofdm_channel(frequencies, a, tau, normalize=False)   # (B,1,1,1,1,S,F) complex64\n",
    "y       = channel_freq(x_rg, h_freq, noise_SC_Watt)                   # (B,1,1,S,F) complex64\n",
    "print(\"Rx Signal Shape :\", y.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Classical baseline labels (ZF + demapper)\n",
    "# ----------------------------\n",
    "# Use **imperfect** or perfect CSI as you prefer. Here we keep perfect for label generation:\n",
    "x_hat, no_eff = zf_equ(y, h_freq, tf.zeros_like(h_freq), noise_SC_Watt)\n",
    "x_hat = x_hat / tf.cast(tf.sqrt(TxPower_SC), x_hat.dtype)\n",
    "llr   = demapper(x_hat, no_eff)                                        # (B, S, F, num_bits_per_symbol)\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare model inputs with SAME shapes you’ve been using\n",
    "#   - input_y : (B,1,S,F) complex64\n",
    "#   - input_h : (B,1,S,F) complex64\n",
    "#   - input_p : (B,1,S,F) complex64\n",
    "# ----------------------------\n",
    "# y is already (B,1,1,S,F) -> squeeze stream axis to get (B,1,S,F)\n",
    "y_data = tf.squeeze(y, axis=2)\n",
    "\n",
    "# h_freq is (B,1,1,1,1,S,F); remove antenna dims -> (B,S,F), then add channel dim -> (B,1,S,F)\n",
    "h_tmp  = tf.squeeze(h_freq, axis=[1,2,3,4])                              # (B,S,F)\n",
    "h_data = tf.expand_dims(h_tmp, axis=1)                                    # (B,1,S,F)\n",
    "\n",
    "# pilots are (B,1,1,S,F) -> squeeze stream axis -> (B,1,S,F)\n",
    "p_data = tf.squeeze(p_rg, axis=2)                                         # (B,1,S,F)\n",
    "\n",
    "# ----------------------------\n",
    "# Labels for training the NN receiver (same as before)\n",
    "# ----------------------------\n",
    "# If your loss expects flattened bits: (B,1,1,-1)\n",
    "llr_probs = tf.sigmoid(llr)\n",
    "llr_probs = tf.reshape(llr_probs, [B, 1, 1, -1])\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"y_data  :\", y_data.shape, y_data.dtype)   # (B,1,S,F) complex64\n",
    "print(\"h_data  :\", h_data.shape, h_data.dtype)   # (B,1,S,F) complex64\n",
    "print(\"p_data  :\", p_data.shape, p_data.dtype)   # (B,1,S,F) complex64\n",
    "print(\"llr_probs:\", llr_probs.shape, llr_probs.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904c6e9-fcd6-440a-810c-796c6887085a",
   "metadata": {},
   "source": [
    "### Neural Rx: NVIDIA Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1f275cd-ccd7-4a1c-b365-69f79307d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 9s 53ms/step - loss: 0.7095 - val_loss: 0.6994\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.6990 - val_loss: 0.6937\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.6607 - val_loss: 0.6144\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.5336 - val_loss: 0.3686\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.3122 - val_loss: 0.2091\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.2236 - val_loss: 0.1664\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1805 - val_loss: 0.1416\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1539 - val_loss: 0.1208\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1374 - val_loss: 0.1085\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1274 - val_loss: 0.0990\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1166 - val_loss: 0.0992\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1112 - val_loss: 0.0936\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1034 - val_loss: 0.0915\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1020 - val_loss: 0.0835\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0969 - val_loss: 0.0827\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0932 - val_loss: 0.0811\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0882 - val_loss: 0.0782\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0859 - val_loss: 0.0756\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0830 - val_loss: 0.0764\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0798 - val_loss: 0.0730\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0765 - val_loss: 0.0681\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0753 - val_loss: 0.0722\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0737 - val_loss: 0.0665\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0729 - val_loss: 0.0629\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0677 - val_loss: 0.0661\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0663 - val_loss: 0.0643\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0644 - val_loss: 0.0611\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 6s 51ms/step - loss: 0.0629 - val_loss: 0.0606\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 6s 51ms/step - loss: 0.0605 - val_loss: 0.0595\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 6s 51ms/step - loss: 0.0591 - val_loss: 0.0642\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0596 - val_loss: 0.0588\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0561 - val_loss: 0.0568\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0546 - val_loss: 0.0570\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0538 - val_loss: 0.0547\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0530 - val_loss: 0.0561\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0519 - val_loss: 0.0577\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0519 - val_loss: 0.0535\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0502 - val_loss: 0.0550\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0500 - val_loss: 0.0534\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0485 - val_loss: 0.0539\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0478 - val_loss: 0.0514\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0470 - val_loss: 0.0536\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0464 - val_loss: 0.0524\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0460 - val_loss: 0.0518\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0452 - val_loss: 0.0524\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0447 - val_loss: 0.0517\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0443 - val_loss: 0.0513\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0436 - val_loss: 0.0520\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0431 - val_loss: 0.0518\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0428 - val_loss: 0.0520\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Layer, LayerNormalization, Dropout, Conv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "# =========================\n",
    "# Helper: dataset split\n",
    "# =========================\n",
    "def ensure_split(y_data, h_data, llr_probs, batch_size=32):\n",
    "    # If split already exists, reuse it\n",
    "    g = globals()\n",
    "    if 'train_indices' in g and 'val_indices' in g:\n",
    "        ti, vi = g['train_indices'], g['val_indices']\n",
    "    else:\n",
    "        dataset_size = y_data.shape[0]\n",
    "        idx = tf.random.shuffle(tf.range(dataset_size))\n",
    "        train_sz = int(0.8 * dataset_size)\n",
    "        ti, vi = idx[:train_sz], idx[train_sz:]\n",
    "        g['train_indices'], g['val_indices'] = ti, vi  # store globally for consistency\n",
    "\n",
    "    y_train = tf.gather(y_data, ti); y_val = tf.gather(y_data, vi)\n",
    "    h_train = tf.gather(h_data, ti); h_val = tf.gather(h_data, vi)\n",
    "    llr_train = tf.gather(llr_probs, ti); llr_val = tf.gather(llr_probs, vi)\n",
    "    return (y_train, y_val, h_train, h_val, llr_train, llr_val, ti, vi)\n",
    "\n",
    "# =========================\n",
    "# Learning-rate schedule\n",
    "# =========================\n",
    "class CustomLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Warmup -> constant -> linear (or quadratic) decay driven by optimizer iterations.\"\"\"\n",
    "    def __init__(self, min_learning_rate, max_learning_rate, iter_limits, quad_decay=False):\n",
    "        super().__init__()\n",
    "        self.quad_decay = quad_decay\n",
    "        self.min_lr = tf.constant(min_learning_rate, tf.float32)\n",
    "        self.max_lr = tf.constant(max_learning_rate, tf.float32)\n",
    "        self.warmup_end = tf.constant(iter_limits[0], tf.float32)\n",
    "        self.decay_start = tf.constant(iter_limits[1], tf.float32)\n",
    "        self.last_iter = tf.constant(iter_limits[2], tf.float32)\n",
    "        if self.quad_decay:\n",
    "            self.dec_slope = self.max_lr / tf.square(self.last_iter - self.decay_start + 1e-12)\n",
    "        else:\n",
    "            self.dec_slope = (self.min_lr - self.max_lr) / (self.last_iter - self.decay_start + 1e-12)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        inc_slope = (self.max_lr - self.min_lr) / (self.warmup_end + 1e-12)\n",
    "        def phase_warmup(): return self.min_lr + step * inc_slope\n",
    "        def phase_const():  return self.max_lr\n",
    "        def phase_decay():\n",
    "            if self.quad_decay:\n",
    "                return self.dec_slope * tf.square(step - self.last_iter)\n",
    "            else:\n",
    "                return self.max_lr + self.dec_slope * (step - self.decay_start)\n",
    "        def phase_zero():   return tf.constant(0.0, tf.float32)\n",
    "        return tf.case([\n",
    "            (step < self.warmup_end, phase_warmup),\n",
    "            (tf.logical_and(step >= self.warmup_end, step < self.decay_start), phase_const),\n",
    "            (tf.logical_and(step >= self.decay_start, step < self.last_iter), phase_decay),\n",
    "        ], default=phase_zero, exclusive=True)\n",
    "\n",
    "# =========================\n",
    "# Loss\n",
    "# =========================\n",
    "def binary_sigmoid_cross_entropy(bit_labels, pred_llr):\n",
    "    bit_labels = tf.cast(bit_labels, pred_llr.dtype)\n",
    "    valid_mask = tf.not_equal(bit_labels, -1)\n",
    "    bit_prob = tf.sigmoid(pred_llr)\n",
    "    bit_prob_masked = tf.boolean_mask(bit_prob, valid_mask)\n",
    "    bit_labels_masked = tf.boolean_mask(bit_labels, valid_mask)\n",
    "    bce = tf.keras.losses.binary_crossentropy(bit_labels_masked, bit_prob_masked)\n",
    "    return tf.reduce_mean(bce)\n",
    "\n",
    "# =========================\n",
    "# Depthwise residual block\n",
    "# =========================\n",
    "class DWResidualBlock(Layer):\n",
    "    def __init__(self, dropout_rate=0.10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg = regularizers.l2(1e-5)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        c = int(input_shape[-1])\n",
    "        self.ln1  = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self.dw1  = DepthwiseConv2D(kernel_size=3, padding='same',\n",
    "                                    activation=None, depth_multiplier=1,\n",
    "                                    depthwise_regularizer=self.reg)\n",
    "        self.pw1  = Conv2D(filters=c, kernel_size=1, padding='same',\n",
    "                           activation=None, kernel_regularizer=self.reg)\n",
    "        self.drop1 = Dropout(self.dropout_rate)\n",
    "\n",
    "        self.ln2  = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self.dw2  = DepthwiseConv2D(kernel_size=3, padding='same',\n",
    "                                    activation=None, depth_multiplier=1,\n",
    "                                    depthwise_regularizer=self.reg)\n",
    "        self.pw2  = Conv2D(filters=c, kernel_size=1, padding='same',\n",
    "                           activation=None, kernel_regularizer=self.reg)\n",
    "        self.drop2 = Dropout(self.dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z = self.ln1(x);  z = relu(z)\n",
    "        z = self.dw1(z);  z = self.pw1(z)\n",
    "        z = self.drop1(z, training=training)\n",
    "\n",
    "        z = self.ln2(z);  z = relu(z)\n",
    "        z = self.dw2(z);  z = self.pw2(z)\n",
    "        z = self.drop2(z, training=training)\n",
    "\n",
    "        return x + z\n",
    "\n",
    "# =========================\n",
    "# Neural Receiver (3 inputs): y, h, p\n",
    "# =========================\n",
    "class NeuralReceiver(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reg = regularizers.l2(1e-5)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_conv  = Conv2D(filters=num_conv_channels, kernel_size=3, padding='same',\n",
    "                                  activation=None, kernel_regularizer=self.reg)\n",
    "        self.res1 = DWResidualBlock(0.10)\n",
    "        self.res2 = DWResidualBlock(0.10)\n",
    "        self.res3 = DWResidualBlock(0.10)\n",
    "        self.res4 = DWResidualBlock(0.10)\n",
    "        self.output_conv = Conv2D(filters=num_bits_per_symbol, kernel_size=3, padding='same',\n",
    "                                  activation=None, kernel_regularizer=self.reg)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Inputs: (B,1,S,F) complex64\n",
    "        y, h, p = inputs\n",
    "        batch_size = tf.shape(y)[0]\n",
    "\n",
    "        # (B,1,S,F) -> (B,S,F,1)\n",
    "        y = tf.transpose(y, [0, 2, 3, 1])\n",
    "        h = tf.transpose(h, [0, 2, 3, 1])\n",
    "        p = tf.transpose(p, [0, 2, 3, 1])\n",
    "\n",
    "        # --- keep only symbol index 1 for h (pilot symbol) ---\n",
    "        symbol_mask = tf.one_hot(1, depth=tf.shape(h)[1], dtype=h.dtype)   # (S,)\n",
    "        symbol_mask = tf.reshape(symbol_mask, [1, -1, 1, 1])               # (1,S,1,1)\n",
    "        h = h * symbol_mask\n",
    "\n",
    "        # Optional: small NMSE noise to h\n",
    "        h_power_scalar = tf.reduce_mean(tf.abs(h)**2)\n",
    "        noise_power = 0.001 * h_power_scalar\n",
    "        n_r = tf.random.normal(tf.shape(h), 0.0, 1.0, dtype=tf.float32)\n",
    "        n_i = tf.random.normal(tf.shape(h), 0.0, 1.0, dtype=tf.float32)\n",
    "        n_c = tf.complex(n_r, n_i)\n",
    "        n_c = n_c / tf.cast(tf.sqrt(tf.reduce_mean(tf.abs(n_c)**2) + 1e-12), tf.complex64)\n",
    "        n_c = n_c * tf.cast(tf.sqrt(noise_power), tf.complex64)\n",
    "        h = h + n_c\n",
    "\n",
    "        # Per-sample power normalization\n",
    "        def norm_per_sample(t):\n",
    "            pow_t = tf.reduce_mean(tf.abs(t), axis=[1,2,3], keepdims=True) + 1e-6\n",
    "            return t / tf.cast(pow_t, t.dtype)\n",
    "\n",
    "        y_n = norm_per_sample(y)\n",
    "        h_n = norm_per_sample(h)\n",
    "        p_n = norm_per_sample(p)\n",
    "\n",
    "        # RI concat -> (B,S,F,6)\n",
    "        y_ri = tf.concat([tf.math.real(y_n), tf.math.imag(y_n)], axis=-1)\n",
    "        h_ri = tf.concat([tf.math.real(h_n), tf.math.imag(h_n)], axis=-1)\n",
    "        p_ri = tf.concat([tf.math.real(p_n), tf.math.imag(p_n)], axis=-1)\n",
    "        z = tf.concat([y_ri, h_ri, p_ri], axis=-1)\n",
    "\n",
    "        # Backbone\n",
    "        z = self.input_conv(z)\n",
    "        z = self.res1(z, training=training)\n",
    "        z = self.res2(z, training=training)\n",
    "        z = self.res3(z, training=training)\n",
    "        z = self.res4(z, training=training)\n",
    "        z = self.output_conv(z)  # (B,S,F,num_bits_per_symbol)\n",
    "\n",
    "        # ---- DROP PILOT SYMBOL (index 1) from logits so labels align ----\n",
    "        # z has S along axis=1: keep [0] and [2..S-1], remove index 1\n",
    "        z_data = tf.concat([z[:, :1, :, :], z[:, 2:, :, :]], axis=1)  # (B,S-1,F,bits)\n",
    "\n",
    "        # Flatten -> (B,1,1,(S-1)*F*bits) to match llr labels\n",
    "        z_flat = tf.reshape(z_data, [batch_size, 1, 1, -1])\n",
    "        return z_flat\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build model (I/O shapes unchanged)\n",
    "# =========================\n",
    "input_y = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_h = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_p = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "\n",
    "receiver = NeuralReceiver()\n",
    "output_llrs = receiver([input_y, input_h, input_p])\n",
    "receiver_model = Model(inputs=[input_y, input_h, input_p], outputs=output_llrs)\n",
    "\n",
    "# =========================\n",
    "# Create pilots p_all (N,1,S,F) complex64, nonzero only at symbol 1\n",
    "# =========================\n",
    "# y_data/h_data/llr_probs must already exist at this point\n",
    "N = int(y_data.shape[0])\n",
    "S = int(num_symbols); F = int(num_SC)\n",
    "\n",
    "p_val = tf.complex(tf.constant(1.0, tf.float32), tf.constant(0.0, tf.float32))  # 1+0j\n",
    "sym1_mask = tf.one_hot(1, depth=S, dtype=tf.float32)       # (S,)\n",
    "sym1_mask = tf.reshape(sym1_mask, [1,1,S,1])               # (1,1,S,1)\n",
    "\n",
    "# All subcarriers as pilots on symbol 1 (comb optional below)\n",
    "sc_mask = tf.ones([1,1,1,F], tf.float32)                   # (1,1,1,F)\n",
    "# # Optional comb pilot pattern:\n",
    "# K = 4\n",
    "# comb = tf.cast(tf.equal(tf.range(F) % K, 0), tf.float32)  # (F,)\n",
    "# sc_mask = tf.reshape(comb, [1,1,1,F])\n",
    "\n",
    "pilot_mask = tf.cast(sym1_mask * sc_mask, tf.complex64)    # (1,1,S,F) complex\n",
    "pilot_mask = tf.tile(pilot_mask, [N,1,1,1])                 # (N,1,S,F)\n",
    "p_all = p_val * pilot_mask                                  # (N,1,S,F) complex64\n",
    "\n",
    "# =========================\n",
    "# Ensure (or create) split, and gather pilots\n",
    "# =========================\n",
    "y_train, y_val, h_train, h_val, llr_train, llr_val, train_indices, val_indices = ensure_split(\n",
    "    y_data, h_data, llr_probs, batch_size=32\n",
    ")\n",
    "p_train = tf.gather(p_all, train_indices)\n",
    "p_val   = tf.gather(p_all, val_indices)\n",
    "\n",
    "# =========================\n",
    "# Compile & Train\n",
    "# =========================\n",
    "batch_size = 32\n",
    "total_epochs = 50\n",
    "steps_per_epoch = math.ceil(y_train.shape[0] / batch_size)\n",
    "\n",
    "warmup_epochs = 6\n",
    "decay_start_epoch = 22\n",
    "iter_limits = [\n",
    "    warmup_epochs * steps_per_epoch,\n",
    "    decay_start_epoch * steps_per_epoch,\n",
    "    total_epochs * steps_per_epoch\n",
    "]\n",
    "\n",
    "lr_schedule = CustomLRSchedule(\n",
    "    min_learning_rate=0.0,\n",
    "    max_learning_rate=1e-3,\n",
    "    iter_limits=iter_limits,\n",
    "    quad_decay=False\n",
    ")\n",
    "\n",
    "receiver_model.compile(\n",
    "    optimizer=Adam(learning_rate=lr_schedule),\n",
    "    loss=binary_sigmoid_cross_entropy\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=6, restore_best_weights=True, min_delta=1e-4\n",
    ")\n",
    "\n",
    "history = receiver_model.fit(\n",
    "    [y_train, h_train, p_train],\n",
    "    llr_train,\n",
    "    validation_data=([y_val, h_val, p_val], llr_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=total_epochs,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40a617fc-b7f6-4294-90ac-6ddadd58eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Receiver BER (validation): 0.040912\n",
      "ZF Baseline BER (validation): 0.032376\n",
      "Relative BER change vs ZF: +26.37%\n"
     ]
    }
   ],
   "source": [
    "# ==== Evaluate after training (validation split) ====\n",
    "\n",
    "# 1) Predict LLRs with the neural receiver  (NOTE: three inputs!)\n",
    "llr_pred = receiver_model.predict([y_val, h_val, p_val], batch_size=32, verbose=0)  # (valN, 1, 1, B_data)\n",
    "\n",
    "# 2) Hard decisions -> predicted bits {0,1}; flatten to (valN, B_data)\n",
    "bits_pred_flat = tf.reshape(tf.cast(llr_pred > 0.0, tf.float32),\n",
    "                            [tf.shape(llr_pred)[0], -1])\n",
    "\n",
    "# 3) Ground-truth bits for the SAME validation subset; flatten to (valN, n_gt_bits)\n",
    "#    b shape from Tx step: (N, num_ut, rg.num_streams_per_tx, n_data_bits)\n",
    "b_all_flat = tf.squeeze(b, axis=[1, 2])                              # -> (N, n_data_bits)\n",
    "b_val_flat = tf.cast(tf.gather(b_all_flat, val_indices), tf.float32) # -> (valN, n_data_bits)\n",
    "\n",
    "# 4) Align lengths in case model outputs only data REs (e.g., pilot symbol dropped)\n",
    "pred_len = tf.shape(bits_pred_flat)[1]\n",
    "gt_len   = tf.shape(b_val_flat)[1]\n",
    "\n",
    "# If needed, truncate ground-truth to the prediction length (should match already if labels were made data-only)\n",
    "b_val_flat = b_val_flat[:, :pred_len]\n",
    "\n",
    "# 5) Neural receiver BER\n",
    "ber_neural = compute_ber(b_val_flat, bits_pred_flat).numpy()\n",
    "print(f\"Neural Receiver BER (validation): {ber_neural:.6f}\")\n",
    "\n",
    "# ----- Optional: compare with classical ZF baseline on the SAME val split -----\n",
    "try:\n",
    "    # If you still have 'llr' from the ZF demapper used earlier:\n",
    "    # llr shape typically: (N, num_symbols, num_SC, num_bits_per_symbol) or already flattened.\n",
    "    if len(llr.shape) > 2:\n",
    "        bits_zf = tf.cast(llr > 0.0, tf.float32)                     # -> (N, ..., ..., ...)\n",
    "        bits_zf_flat = tf.reshape(bits_zf, [tf.shape(bits_zf)[0], -1])  # -> (N, n_bits_total)\n",
    "    else:\n",
    "        bits_zf_flat = tf.cast(llr > 0.0, tf.float32)                # already (N, n_bits_total)\n",
    "\n",
    "    bits_zf_val_flat = tf.gather(bits_zf_flat, val_indices)          # -> (valN, n_bits_total)\n",
    "\n",
    "    # Align ZF length with model output (data-only length)\n",
    "    bits_zf_val_flat = bits_zf_val_flat[:, :pred_len]\n",
    "\n",
    "    ber_zf = compute_ber(b_val_flat, bits_zf_val_flat).numpy()\n",
    "    print(f\"ZF Baseline BER (validation): {ber_zf:.6f}\")\n",
    "    if ber_zf > 0:\n",
    "        print(f\"Relative BER change vs ZF: {(ber_neural - ber_zf) / ber_zf:+.2%}\")\n",
    "except Exception as e:\n",
    "    print(\"ZF baseline comparison skipped (llr not available or shape mismatch).\")\n",
    "    print(f\"Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd5621fd-a806-4178-bc8f-b9869788c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_all shape (expect N,1,1,(S-1)*F*Bbits): (5000, 1, 1, 2496)\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 10s 53ms/step - loss: 0.7233 - val_loss: 0.6997\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 6s 52ms/step - loss: 0.7007 - val_loss: 0.6979\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.6884 - val_loss: 0.6469\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.6060 - val_loss: 0.4788\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.3907 - val_loss: 0.2600\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.2649 - val_loss: 0.2052\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.2161 - val_loss: 0.1728\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1909 - val_loss: 0.1578\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1734 - val_loss: 0.1469\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1610 - val_loss: 0.1371\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1524 - val_loss: 0.1347\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1465 - val_loss: 0.1321\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1404 - val_loss: 0.1278\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1372 - val_loss: 0.1268\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1352 - val_loss: 0.1249\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1314 - val_loss: 0.1212\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1279 - val_loss: 0.1216\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1262 - val_loss: 0.1184\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1245 - val_loss: 0.1198\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1229 - val_loss: 0.1172\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1201 - val_loss: 0.1191\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1182 - val_loss: 0.1170\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1164 - val_loss: 0.1137\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1157 - val_loss: 0.1120\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1123 - val_loss: 0.1097\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1108 - val_loss: 0.1104\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1092 - val_loss: 0.1086\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1079 - val_loss: 0.1093\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1058 - val_loss: 0.1066\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1050 - val_loss: 0.1081\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1040 - val_loss: 0.1050\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1014 - val_loss: 0.1046\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1005 - val_loss: 0.1035\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0993 - val_loss: 0.1033\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0980 - val_loss: 0.1018\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0971 - val_loss: 0.1018\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0963 - val_loss: 0.1013\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0961 - val_loss: 0.0996\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0948 - val_loss: 0.0998\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0941 - val_loss: 0.0996\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0934 - val_loss: 0.1000\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0929 - val_loss: 0.0992\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0924 - val_loss: 0.1016\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0920 - val_loss: 0.0990\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0912 - val_loss: 0.0990\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0909 - val_loss: 0.0997\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0905 - val_loss: 0.0989\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0901 - val_loss: 0.0988\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0897 - val_loss: 0.0991\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0894 - val_loss: 0.0988\n",
      "pred_val shape: (1000, 1, 1, 2496)  expected last dim: 2496\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Layer, LayerNormalization, Dropout, Conv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "# =========================\n",
    "# Config / Safety defaults (non-structural)\n",
    "# =========================\n",
    "# If these are already defined in your notebook, these lines are harmless.\n",
    "num_conv_channels = globals().get('num_conv_channels', 64)   # only a default, uses existing if set\n",
    "PILOT_SYM        = globals().get('PILOT_SYM', 1)\n",
    "\n",
    "# Short-hands (taken from your environment)\n",
    "S      = int(num_symbols)\n",
    "F      = int(num_SC)\n",
    "Bbits  = int(num_bits_per_symbol)\n",
    "N      = int(y_data.shape[0])\n",
    "\n",
    "# =========================\n",
    "# Helper: dataset split (unchanged)\n",
    "# =========================\n",
    "def ensure_split(y_data, h_data, labels, batch_size=32):\n",
    "    # If split already exists, reuse it\n",
    "    g = globals()\n",
    "    if 'train_indices' in g and 'val_indices' in g:\n",
    "        ti, vi = g['train_indices'], g['val_indices']\n",
    "    else:\n",
    "        dataset_size = y_data.shape[0]\n",
    "        idx = tf.random.shuffle(tf.range(dataset_size))\n",
    "        train_sz = int(0.8 * dataset_size)\n",
    "        ti, vi = idx[:train_sz], idx[train_sz:]\n",
    "        g['train_indices'], g['val_indices'] = ti, vi  # store globally for consistency\n",
    "\n",
    "    y_train = tf.gather(y_data, ti); y_val = tf.gather(y_data, vi)\n",
    "    h_train = tf.gather(h_data, ti); h_val = tf.gather(h_data, vi)\n",
    "    l_train = tf.gather(labels, ti); l_val = tf.gather(labels, vi)\n",
    "    return (y_train, y_val, h_train, h_val, l_train, l_val, ti, vi)\n",
    "\n",
    "# =========================\n",
    "# Learning-rate schedule (unchanged)\n",
    "# =========================\n",
    "class CustomLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Warmup -> constant -> linear (or quadratic) decay driven by optimizer iterations.\"\"\"\n",
    "    def __init__(self, min_learning_rate, max_learning_rate, iter_limits, quad_decay=False):\n",
    "        super().__init__()\n",
    "        self.quad_decay = quad_decay\n",
    "        self.min_lr = tf.constant(min_learning_rate, tf.float32)\n",
    "        self.max_lr = tf.constant(max_learning_rate, tf.float32)\n",
    "        self.warmup_end = tf.constant(iter_limits[0], tf.float32)\n",
    "        self.decay_start = tf.constant(iter_limits[1], tf.float32)\n",
    "        self.last_iter = tf.constant(iter_limits[2], tf.float32)\n",
    "        if self.quad_decay:\n",
    "            self.dec_slope = self.max_lr / tf.square(self.last_iter - self.decay_start + 1e-12)\n",
    "        else:\n",
    "            self.dec_slope = (self.min_lr - self.max_lr) / (self.last_iter - self.decay_start + 1e-12)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        inc_slope = (self.max_lr - self.min_lr) / (self.warmup_end + 1e-12)\n",
    "        def phase_warmup(): return self.min_lr + step * inc_slope\n",
    "        def phase_const():  return self.max_lr\n",
    "        def phase_decay():\n",
    "            if self.quad_decay:\n",
    "                return self.dec_slope * tf.square(step - self.last_iter)\n",
    "            else:\n",
    "                return self.max_lr + self.dec_slope * (step - self.decay_start)\n",
    "        def phase_zero():   return tf.constant(0.0, tf.float32)\n",
    "        return tf.case([\n",
    "            (step < self.warmup_end, phase_warmup),\n",
    "            (tf.logical_and(step >= self.warmup_end, step < self.decay_start), phase_const),\n",
    "            (tf.logical_and(step >= self.decay_start, step < self.last_iter), phase_decay),\n",
    "        ], default=phase_zero, exclusive=True)\n",
    "\n",
    "# =========================\n",
    "# Loss (unchanged; accepts {0,1} and supports -1 masking)\n",
    "# =========================\n",
    "def binary_sigmoid_cross_entropy(bit_labels, pred_llr):\n",
    "    bit_labels = tf.cast(bit_labels, pred_llr.dtype)\n",
    "    valid_mask = tf.not_equal(bit_labels, -1)\n",
    "    bit_prob = tf.sigmoid(pred_llr)\n",
    "    bit_prob_masked = tf.boolean_mask(bit_prob, valid_mask)\n",
    "    bit_labels_masked = tf.boolean_mask(bit_labels, valid_mask)\n",
    "    bce = tf.keras.losses.binary_crossentropy(bit_labels_masked, bit_prob_masked)\n",
    "    return tf.reduce_mean(bce)\n",
    "\n",
    "# =========================\n",
    "# Depthwise residual block (unchanged)\n",
    "# =========================\n",
    "class DWResidualBlock(Layer):\n",
    "    def __init__(self, dropout_rate=0.10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg = regularizers.l2(1e-5)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        c = int(input_shape[-1])\n",
    "        self.ln1  = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self.dw1  = DepthwiseConv2D(kernel_size=3, padding='same',\n",
    "                                    activation=None, depth_multiplier=1,\n",
    "                                    depthwise_regularizer=self.reg)\n",
    "        self.pw1  = Conv2D(filters=c, kernel_size=1, padding='same',\n",
    "                           activation=None, kernel_regularizer=self.reg)\n",
    "        self.drop1 = Dropout(self.dropout_rate)\n",
    "\n",
    "        self.ln2  = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self.dw2  = DepthwiseConv2D(kernel_size=3, padding='same',\n",
    "                                    activation=None, depth_multiplier=1,\n",
    "                                    depthwise_regularizer=self.reg)\n",
    "        self.pw2  = Conv2D(filters=c, kernel_size=1, padding='same',\n",
    "                           activation=None, kernel_regularizer=self.reg)\n",
    "        self.drop2 = Dropout(self.dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z = self.ln1(x);  z = relu(z)\n",
    "        z = self.dw1(z);  z = self.pw1(z)\n",
    "        z = self.drop1(z, training=training)\n",
    "\n",
    "        z = self.ln2(z);  z = relu(z)\n",
    "        z = self.dw2(z);  z = self.pw2(z)\n",
    "        z = self.drop2(z, training=training)\n",
    "\n",
    "        return x + z\n",
    "\n",
    "# =========================\n",
    "# Neural Receiver (unchanged topology, keeps pilot masking inside)\n",
    "# =========================\n",
    "class NeuralReceiver(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reg = regularizers.l2(1e-5)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_conv  = Conv2D(filters=num_conv_channels, kernel_size=3, padding='same',\n",
    "                                  activation=None, kernel_regularizer=self.reg)\n",
    "        self.res1 = DWResidualBlock(0.10)\n",
    "        self.res2 = DWResidualBlock(0.10)\n",
    "        self.res3 = DWResidualBlock(0.10)\n",
    "        self.res4 = DWResidualBlock(0.10)\n",
    "        self.output_conv = Conv2D(filters=num_bits_per_symbol, kernel_size=3, padding='same',\n",
    "                                  activation=None, kernel_regularizer=self.reg)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Inputs: (B,1,S,F) complex64\n",
    "        y, h, p = inputs\n",
    "        batch_size = tf.shape(y)[0]\n",
    "\n",
    "        # (B,1,S,F) -> (B,S,F,1)\n",
    "        y = tf.transpose(y, [0, 2, 3, 1])\n",
    "        h = tf.transpose(h, [0, 2, 3, 1])\n",
    "        p = tf.transpose(p, [0, 2, 3, 1])\n",
    "\n",
    "        # --- keep only PILOT_SYM for h (pilot symbol) ---\n",
    "        symbol_mask = tf.one_hot(PILOT_SYM, depth=tf.shape(h)[1], dtype=h.dtype)  # (S,)\n",
    "        symbol_mask = tf.reshape(symbol_mask, [1, -1, 1, 1])                      # (1,S,1,1)\n",
    "        h = h * symbol_mask\n",
    "\n",
    "        # Optional: small NMSE noise to h (regularization)\n",
    "        h_power_scalar = tf.reduce_mean(tf.abs(h)**2)\n",
    "        noise_power = 0.001 * h_power_scalar\n",
    "        n_r = tf.random.normal(tf.shape(h), 0.0, 1.0, dtype=tf.float32)\n",
    "        n_i = tf.random.normal(tf.shape(h), 0.0, 1.0, dtype=tf.float32)\n",
    "        n_c = tf.complex(n_r, n_i)\n",
    "        n_c = n_c / tf.cast(tf.sqrt(tf.reduce_mean(tf.abs(n_c)**2) + 1e-12), tf.complex64)\n",
    "        n_c = n_c * tf.cast(tf.sqrt(noise_power), tf.complex64)\n",
    "        h = h + n_c\n",
    "\n",
    "        # Per-sample power normalization\n",
    "        def norm_per_sample(t):\n",
    "            pow_t = tf.reduce_mean(tf.abs(t), axis=[1,2,3], keepdims=True) + 1e-6\n",
    "            return t / tf.cast(pow_t, t.dtype)\n",
    "\n",
    "        y_n = norm_per_sample(y)\n",
    "        h_n = norm_per_sample(h)\n",
    "        p_n = norm_per_sample(p)\n",
    "\n",
    "        # RI concat -> (B,S,F,6)\n",
    "        y_ri = tf.concat([tf.math.real(y_n), tf.math.imag(y_n)], axis=-1)\n",
    "        h_ri = tf.concat([tf.math.real(h_n), tf.math.imag(h_n)], axis=-1)\n",
    "        p_ri = tf.concat([tf.math.real(p_n), tf.math.imag(p_n)], axis=-1)\n",
    "        z = tf.concat([y_ri, h_ri, p_ri], axis=-1)\n",
    "\n",
    "        # Backbone\n",
    "        z = self.input_conv(z)\n",
    "        z = self.res1(z, training=training)\n",
    "        z = self.res2(z, training=training)\n",
    "        z = self.res3(z, training=training)\n",
    "        z = self.res4(z, training=training)\n",
    "        z = self.output_conv(z)  # (B,S,F,num_bits_per_symbol)\n",
    "\n",
    "        # ---- DROP PILOT SYMBOL (index = PILOT_SYM) from logits so labels align ----\n",
    "        z_data = tf.concat([z[:, :PILOT_SYM, :, :], z[:, PILOT_SYM+1:, :, :]], axis=1)  # (B,S-1,F,bits)\n",
    "\n",
    "        # Flatten -> (B,1,1,(S-1)*F*bits) to match labels\n",
    "        z_flat = tf.reshape(z_data, [batch_size, 1, 1, -1])\n",
    "        return z_flat\n",
    "\n",
    "# =========================\n",
    "# Build model (unchanged I/O)\n",
    "# =========================\n",
    "input_y = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_h = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_p = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "\n",
    "receiver = NeuralReceiver()\n",
    "output_llrs = receiver([input_y, input_h, input_p])\n",
    "receiver_model = Model(inputs=[input_y, input_h, input_p], outputs=output_llrs)\n",
    "\n",
    "# =========================\n",
    "# Create pilots p_all (N,1,S,F) complex64, nonzero only at symbol PILOT_SYM (unchanged)\n",
    "# =========================\n",
    "p_val = tf.complex(tf.constant(1.0, tf.float32), tf.constant(0.0, tf.float32))  # 1+0j\n",
    "sym_mask = tf.one_hot(PILOT_SYM, depth=S, dtype=tf.float32)  # (S,)\n",
    "sym_mask = tf.reshape(sym_mask, [1,1,S,1])                   # (1,1,S,1)\n",
    "sc_mask = tf.ones([1,1,1,F], tf.float32)                     # (1,1,1,F) (all pilots on that symbol)\n",
    "pilot_mask = tf.cast(sym_mask * sc_mask, tf.complex64)       # (1,1,S,F)\n",
    "pilot_mask = tf.tile(pilot_mask, [N,1,1,1])                  # (N,1,S,F)\n",
    "p_all = p_val * pilot_mask                                   # (N,1,S,F) complex64\n",
    "\n",
    "# =========================\n",
    "# TRUE-BIT labels from b (data-only, pilot excluded)\n",
    "# =========================\n",
    "# b: (N, num_ut, rg.num_streams_per_tx, n) with n == (S-1)*F*Bbits\n",
    "def true_bit_labels_from_b(b, S, F, Bbits):\n",
    "    b = tf.cast(b, tf.float32)                              # {0,1}\n",
    "    b_flat = tf.reshape(b, [tf.shape(b)[0], -1])            # (N, (S-1)*F*Bbits)\n",
    "    return tf.reshape(b_flat, [tf.shape(b_flat)[0], 1, 1, (S-1)*F*Bbits])\n",
    "\n",
    "labels_all = true_bit_labels_from_b(b, S, F, Bbits)\n",
    "print(\"labels_all shape (expect N,1,1,(S-1)*F*Bbits):\", labels_all.shape)\n",
    "\n",
    "# Safety check: length matches model output’s last dim\n",
    "exp_len = (S-1)*F*Bbits\n",
    "assert int(labels_all.shape[-1]) == exp_len, \\\n",
    "    f\"Label length {int(labels_all.shape[-1])} != expected {(S-1)*F*Bbits}. Check that n == (S-1)*F*Bbits at the transmitter.\"\n",
    "\n",
    "# =========================\n",
    "# Train/Val split (and align pilots with the split)\n",
    "# =========================\n",
    "y_train, y_val, h_train, h_val, l_train, l_val, train_idx, val_idx = ensure_split(\n",
    "    y_data, h_data, labels_all, batch_size=32\n",
    ")\n",
    "p_train = tf.gather(p_all, train_idx)\n",
    "p_val   = tf.gather(p_all, val_idx)\n",
    "\n",
    "# =========================\n",
    "# Compile & Train (optimizer/schedule unchanged)\n",
    "# =========================\n",
    "batch_size = 32\n",
    "total_epochs = 50\n",
    "steps_per_epoch = math.ceil(y_train.shape[0] / batch_size)\n",
    "\n",
    "warmup_epochs = 6\n",
    "decay_start_epoch = 22\n",
    "iter_limits = [\n",
    "    warmup_epochs * steps_per_epoch,\n",
    "    decay_start_epoch * steps_per_epoch,\n",
    "    total_epochs * steps_per_epoch\n",
    "]\n",
    "\n",
    "lr_schedule = CustomLRSchedule(\n",
    "    min_learning_rate=0.0,\n",
    "    max_learning_rate=1e-3,\n",
    "    iter_limits=iter_limits,\n",
    "    quad_decay=False\n",
    ")\n",
    "\n",
    "receiver_model.compile(\n",
    "    optimizer=Adam(learning_rate=lr_schedule),\n",
    "    loss=binary_sigmoid_cross_entropy\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=6, restore_best_weights=True, min_delta=1e-4\n",
    ")\n",
    "\n",
    "history = receiver_model.fit(\n",
    "    [y_train, h_train, p_train],\n",
    "    l_train,  # <<<<<< TRUE-BIT LABELS\n",
    "    validation_data=([y_val, h_val, p_val], l_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=total_epochs,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Quick sanity check on alignment\n",
    "# =========================\n",
    "pred_val = receiver_model.predict([y_val, h_val, p_val], batch_size=batch_size, verbose=0)\n",
    "print(\"pred_val shape:\", pred_val.shape, \" expected last dim:\", exp_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3a62d095-198e-478e-859a-b6de075bae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Classical BER: 0.032376\n",
      "[VAL] Neural     BER: 0.042948\n",
      "[VAL] Absolute BER improvement: -0.010572\n",
      "[VAL] Relative BER improvement: -32.65%\n",
      "[VAL] Classical BCE vs true bits: 0.492639\n",
      "[VAL] Neural     BCE vs true bits: 0.092734\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Evaluation: Neural RX vs Classical RX\n",
    "# =========================\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---- helpers ----\n",
    "def flatten_4d_last(x):\n",
    "    \"\"\"(N,1,1,L) -> (N,L)\"\"\"\n",
    "    return tf.reshape(x, [tf.shape(x)[0], -1])\n",
    "\n",
    "def compute_ber(bits_true_flat, bits_pred_flat):\n",
    "    \"\"\"Both tensors are float {0,1} with shape (N,L).\"\"\"\n",
    "    bits_true_flat = tf.cast(bits_true_flat, tf.float32)\n",
    "    bits_pred_flat = tf.cast(bits_pred_flat, tf.float32)\n",
    "    mismatches = tf.not_equal(bits_true_flat, bits_pred_flat)\n",
    "    return tf.reduce_mean(tf.cast(mismatches, tf.float32))\n",
    "\n",
    "def grid_drop_pilot_and_flatten(grid, S, F, Bbits, pilot_sym_index):\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      - (N, S, F, Bbits) grid  (full including pilot), or\n",
    "      - (N, 1, 1, (S-1)*F*Bbits) already data-only flattened.\n",
    "    Returns (N, 1, 1, (S-1)*F*Bbits)\n",
    "    \"\"\"\n",
    "    gshape = tf.shape(grid)\n",
    "    rank = grid.shape.rank\n",
    "\n",
    "    exp_len_full  = S * F * Bbits\n",
    "    exp_len_data  = (S - 1) * F * Bbits\n",
    "    last_dim      = tf.shape(grid)[-1]\n",
    "\n",
    "    # Case A: already data-only flat\n",
    "    if rank == 4 and grid.shape[1] == 1 and grid.shape[2] == 1 and int(grid.shape[-1]) == exp_len_data:\n",
    "        return grid\n",
    "\n",
    "    # Case B: full grid (N,S,F,Bbits)\n",
    "    # Try to reshape if we were given flattened full length\n",
    "    if rank == 4 and grid.shape[1] == 1 and grid.shape[2] == 1 and int(grid.shape[-1]) == exp_len_full:\n",
    "        N = tf.shape(grid)[0]\n",
    "        grid_full = tf.reshape(grid, [N, S, F, Bbits])\n",
    "    elif rank == 4 and (grid.shape[1] == S and grid.shape[2] == F and grid.shape[3] == Bbits):\n",
    "        grid_full = grid\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected label/prob shape {grid.shape}. \"\n",
    "            f\"Expected (N,S,F,Bbits) or (N,1,1,{exp_len_full}) or (N,1,1,{exp_len_data}).\"\n",
    "        )\n",
    "\n",
    "    # Drop the pilot symbol along axis=1\n",
    "    grid_data = tf.concat([grid_full[:, :pilot_sym_index, :, :],\n",
    "                           grid_full[:, pilot_sym_index+1:, :, :]], axis=1)  # (N,S-1,F,Bbits)\n",
    "    # Flatten back\n",
    "    N = tf.shape(grid_data)[0]\n",
    "    flat = tf.reshape(grid_data, [N, 1, 1, (S - 1) * F * Bbits])\n",
    "    return flat\n",
    "\n",
    "# ---------------------------------\n",
    "# Classical RX BER on the same val set\n",
    "# ---------------------------------\n",
    "# We use llr_probs (from your earlier ZF+Demapper pipeline).\n",
    "# 1) Gather val subset\n",
    "llr_val_full = tf.gather(llr_probs, val_idx)\n",
    "\n",
    "# 2) Ensure shape -> drop the pilot symbol -> flatten to (N,1,1,(S-1)*F*Bbits)\n",
    "llr_val_data_flat = grid_drop_pilot_and_flatten(llr_val_full, S, F, Bbits, PILOT_SYM)\n",
    "\n",
    "# 3) Threshold classical probabilities at 0.5 -> hard bits\n",
    "classical_bits_val_flat = tf.cast(llr_val_data_flat > 0.5, tf.float32)\n",
    "\n",
    "# 4) True bits (already flat & data-only): l_val\n",
    "true_bits_val_flat = l_val  # shape (N,1,1,(S-1)*F*Bbits)\n",
    "\n",
    "# Convert both to (N,L) for BER\n",
    "classical_bits_val = flatten_4d_last(classical_bits_val_flat)\n",
    "true_bits_val      = flatten_4d_last(true_bits_val_flat)\n",
    "\n",
    "ber_classical_val = compute_ber(true_bits_val, classical_bits_val)\n",
    "\n",
    "# ---------------------------------\n",
    "# Neural RX BER on the same val set\n",
    "# ---------------------------------\n",
    "# 1) Predict logits -> convert to probs/logits\n",
    "pred_val_logits = receiver_model.predict([y_val, h_val, p_val], batch_size=32, verbose=0)  # (N,1,1,L)\n",
    "pred_val_probs  = tf.sigmoid(pred_val_logits)\n",
    "\n",
    "# 2) Hard decisions\n",
    "neural_bits_val_flat = tf.cast(pred_val_probs > 0.5, tf.float32)           # (N,1,1,L)\n",
    "neural_bits_val      = flatten_4d_last(neural_bits_val_flat)\n",
    "ber_neural_val       = compute_ber(true_bits_val, neural_bits_val)\n",
    "\n",
    "# ---------------------------------\n",
    "# (Optional) Also compute losses against true bits\n",
    "# ---------------------------------\n",
    "val_loss_neural = binary_sigmoid_cross_entropy(true_bits_val_flat, pred_val_logits).numpy()\n",
    "# For a fair comparison, compute classical loss by turning classical probs into \"logits-like\" inputs.\n",
    "# We can pass probabilities through tf.math.log to form logits; safer is BCE directly with probs:\n",
    "val_loss_classical = tf.reduce_mean(\n",
    "    tf.keras.losses.binary_crossentropy(\n",
    "        flatten_4d_last(true_bits_val_flat),  # y_true\n",
    "        flatten_4d_last(llr_val_data_flat)    # y_pred as probs\n",
    "    )\n",
    ").numpy()\n",
    "\n",
    "# ---------------------------------\n",
    "# Print summary\n",
    "# ---------------------------------\n",
    "print(f\"[VAL] Classical BER: {ber_classical_val.numpy():.6f}\")\n",
    "print(f\"[VAL] Neural     BER: {ber_neural_val.numpy():.6f}\")\n",
    "impr = (ber_classical_val - ber_neural_val).numpy()\n",
    "rel  = (impr / (ber_classical_val + 1e-12)).numpy()\n",
    "print(f\"[VAL] Absolute BER improvement: {impr:.6f}\")\n",
    "print(f\"[VAL] Relative BER improvement: {100.0*rel:.2f}%\")\n",
    "print(f\"[VAL] Classical BCE vs true bits: {val_loss_classical:.6f}\")\n",
    "print(f\"[VAL] Neural     BCE vs true bits: {val_loss_neural:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8e3880ef-2348-4433-8bef-ee3bca3c7ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_all shape (expect N,1,1,(S-1)*F*Bbits): (5000, 1, 1, 2496)\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 9s 52ms/step - loss: 0.6776 - val_loss: 0.5165\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.3323 - val_loss: 0.1705\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1729 - val_loss: 0.1340\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1488 - val_loss: 0.1275\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1389 - val_loss: 0.1204\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1309 - val_loss: 0.1176\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1219 - val_loss: 0.1037\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1143 - val_loss: 0.1007\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1095 - val_loss: 0.0974\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1058 - val_loss: 0.0962\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1026 - val_loss: 0.0925\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.1004 - val_loss: 0.0907\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0975 - val_loss: 0.0896\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0959 - val_loss: 0.0893\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0942 - val_loss: 0.0885\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0927 - val_loss: 0.0875\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0917 - val_loss: 0.0875\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0902 - val_loss: 0.0873\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0894 - val_loss: 0.0859\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0884 - val_loss: 0.0868\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0876 - val_loss: 0.0888\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0872 - val_loss: 0.0865\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0862 - val_loss: 0.0850\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0856 - val_loss: 0.0885\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0850 - val_loss: 0.0864\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0836 - val_loss: 0.0857\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0833 - val_loss: 0.0842\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0823 - val_loss: 0.0850\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0820 - val_loss: 0.0856\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0814 - val_loss: 0.0837\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0808 - val_loss: 0.0860\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0801 - val_loss: 0.0837\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0796 - val_loss: 0.0855\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0794 - val_loss: 0.0843\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 6s 49ms/step - loss: 0.0795 - val_loss: 0.0879\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0785 - val_loss: 0.0856\n",
      "[VAL] Classical BER: 0.032373\n",
      "[VAL] Neural BER @ best threshold 0.510: 0.038430\n",
      "[VAL] Absolute BER improvement: -0.006057\n",
      "[VAL] Relative BER improvement: -18.71%\n",
      "[VAL] Neural BCE vs true bits: 0.080140\n",
      "[VAL] Neural BER @ threshold 0.500: 0.038442\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Layer, LayerNormalization, Dropout, Conv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "# =========================\n",
    "# Config / Safety defaults (non-structural)\n",
    "# =========================\n",
    "num_conv_channels = globals().get('num_conv_channels', 64)   # keep topology the same\n",
    "PILOT_SYM        = globals().get('PILOT_SYM', 1)\n",
    "\n",
    "# Short-hands (taken from your environment)\n",
    "S      = int(num_symbols)\n",
    "F      = int(num_SC)\n",
    "Bbits  = int(num_bits_per_symbol)\n",
    "N      = int(y_data.shape[0])\n",
    "TxP    = float(globals().get('TxPower_SC', 1.0))  # default to 1 if not defined\n",
    "\n",
    "assert Bbits == 2, \"This evaluation code assumes QPSK (num_bits_per_symbol == 2).\"\n",
    "\n",
    "# =========================\n",
    "# Helper: dataset split (unchanged)\n",
    "# =========================\n",
    "def ensure_split(y_data, h_data, labels, batch_size=32):\n",
    "    # If split already exists, reuse it\n",
    "    g = globals()\n",
    "    if 'train_indices' in g and 'val_indices' in g:\n",
    "        ti, vi = g['train_indices'], g['val_indices']\n",
    "    else:\n",
    "        dataset_size = y_data.shape[0]\n",
    "        idx = tf.random.shuffle(tf.range(dataset_size))\n",
    "        train_sz = int(0.8 * dataset_size)\n",
    "        ti, vi = idx[:train_sz], idx[train_sz:]\n",
    "        g['train_indices'], g['val_indices'] = ti, vi  # store globally for consistency\n",
    "\n",
    "    y_train = tf.gather(y_data, ti); y_val = tf.gather(y_data, vi)\n",
    "    h_train = tf.gather(h_data, ti); h_val = tf.gather(h_data, vi)\n",
    "    l_train = tf.gather(labels, ti); l_val = tf.gather(labels, vi)\n",
    "    return (y_train, y_val, h_train, h_val, l_train, l_val, ti, vi)\n",
    "\n",
    "# =========================\n",
    "# Learning-rate schedule (unchanged)\n",
    "# =========================\n",
    "class CustomLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Warmup -> constant -> linear (or quadratic) decay driven by optimizer iterations.\"\"\"\n",
    "    def __init__(self, min_learning_rate, max_learning_rate, iter_limits, quad_decay=False):\n",
    "        super().__init__()\n",
    "        self.quad_decay = quad_decay\n",
    "        self.min_lr = tf.constant(min_learning_rate, tf.float32)\n",
    "        self.max_lr = tf.constant(max_learning_rate, tf.float32)\n",
    "        self.warmup_end = tf.constant(iter_limits[0], tf.float32)\n",
    "        self.decay_start = tf.constant(iter_limits[1], tf.float32)\n",
    "        self.last_iter = tf.constant(iter_limits[2], tf.float32)\n",
    "        if self.quad_decay:\n",
    "            self.dec_slope = self.max_lr / tf.square(self.last_iter - self.decay_start + 1e-12)\n",
    "        else:\n",
    "            self.dec_slope = (self.min_lr - self.max_lr) / (self.last_iter - self.decay_start + 1e-12)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        inc_slope = (self.max_lr - self.min_lr) / (self.warmup_end + 1e-12)\n",
    "        def phase_warmup(): return self.min_lr + step * inc_slope\n",
    "        def phase_const():  return self.max_lr\n",
    "        def phase_decay():\n",
    "            if self.quad_decay:\n",
    "                return self.dec_slope * tf.square(step - self.last_iter)\n",
    "            else:\n",
    "                return self.max_lr + self.dec_slope * (step - self.decay_start)\n",
    "        def phase_zero():   return tf.constant(0.0, tf.float32)\n",
    "        return tf.case([\n",
    "            (step < self.warmup_end, phase_warmup),\n",
    "            (tf.logical_and(step >= self.warmup_end, step < self.decay_start), phase_const),\n",
    "            (tf.logical_and(step >= self.decay_start, step < self.last_iter), phase_decay),\n",
    "        ], default=phase_zero, exclusive=True)\n",
    "\n",
    "# =========================\n",
    "# Loss (unchanged; accepts {0,1} and supports -1 masking)\n",
    "# =========================\n",
    "def binary_sigmoid_cross_entropy(bit_labels, pred_llr):\n",
    "    bit_labels = tf.cast(bit_labels, pred_llr.dtype)\n",
    "    valid_mask = tf.not_equal(bit_labels, -1)\n",
    "    bit_prob = tf.sigmoid(pred_llr)\n",
    "    bit_prob_masked = tf.boolean_mask(bit_prob, valid_mask)\n",
    "    bit_labels_masked = tf.boolean_mask(bit_labels, valid_mask)\n",
    "    bce = tf.keras.losses.binary_crossentropy(bit_labels_masked, bit_prob_masked)\n",
    "    return tf.reduce_mean(bce)\n",
    "\n",
    "# =========================\n",
    "# Depthwise residual block (unchanged)\n",
    "# =========================\n",
    "class DWResidualBlock(Layer):\n",
    "    def __init__(self, dropout_rate=0.10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg = regularizers.l2(1e-5)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        c = int(input_shape[-1])\n",
    "        self.ln1  = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self.dw1  = DepthwiseConv2D(kernel_size=3, padding='same',\n",
    "                                    activation=None, depth_multiplier=1,\n",
    "                                    depthwise_regularizer=self.reg)\n",
    "        self.pw1  = Conv2D(filters=c, kernel_size=1, padding='same',\n",
    "                           activation=None, kernel_regularizer=self.reg)\n",
    "        self.drop1 = Dropout(self.dropout_rate)\n",
    "\n",
    "        self.ln2  = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self.dw2  = DepthwiseConv2D(kernel_size=3, padding='same',\n",
    "                                    activation=None, depth_multiplier=1,\n",
    "                                    depthwise_regularizer=self.reg)\n",
    "        self.pw2  = Conv2D(filters=c, kernel_size=1, padding='same',\n",
    "                           activation=None, kernel_regularizer=self.reg)\n",
    "        self.drop2 = Dropout(self.dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z = self.ln1(x);  z = relu(z)\n",
    "        z = self.dw1(z);  z = self.pw1(z)\n",
    "        z = self.drop1(z, training=training)\n",
    "\n",
    "        z = self.ln2(z);  z = relu(z)\n",
    "        z = self.dw2(z);  z = self.pw2(z)\n",
    "        z = self.drop2(z, training=training)\n",
    "\n",
    "        return x + z\n",
    "\n",
    "# =========================\n",
    "# Neural Receiver (unchanged topology, keeps pilot masking inside)\n",
    "# =========================\n",
    "class NeuralReceiver(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reg = regularizers.l2(1e-5)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_conv  = Conv2D(filters=num_conv_channels, kernel_size=3, padding='same',\n",
    "                                  activation=None, kernel_regularizer=self.reg)\n",
    "        self.res1 = DWResidualBlock(0.10)\n",
    "        self.res2 = DWResidualBlock(0.10)\n",
    "        self.res3 = DWResidualBlock(0.10)\n",
    "        self.res4 = DWResidualBlock(0.10)\n",
    "        self.output_conv = Conv2D(filters=num_bits_per_symbol, kernel_size=3, padding='same',\n",
    "                                  activation=None, kernel_regularizer=self.reg)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Inputs: (B,1,S,F) complex64\n",
    "        y, h, p = inputs\n",
    "        batch_size = tf.shape(y)[0]\n",
    "\n",
    "        # (B,1,S,F) -> (B,S,F,1)\n",
    "        y = tf.transpose(y, [0, 2, 3, 1])\n",
    "        h = tf.transpose(h, [0, 2, 3, 1])\n",
    "        p = tf.transpose(p, [0, 2, 3, 1])\n",
    "\n",
    "        # No masking of h — give full CSI\n",
    "        eps = tf.complex(tf.constant(1e-6, tf.float32), tf.constant(0.0, tf.float32))\n",
    "        s_eq = y / (h + eps)  # ZF equalized symbols\n",
    "\n",
    "        # Feature stack (RI split)\n",
    "        y_ri    = tf.concat([tf.math.real(y),    tf.math.imag(y)],    axis=-1)  # (B,S,F,2)\n",
    "        h_ri    = tf.concat([tf.math.real(h),    tf.math.imag(h)],    axis=-1)  # (B,S,F,2)\n",
    "        s_eq_ri = tf.concat([tf.math.real(s_eq), tf.math.imag(s_eq)], axis=-1)  # (B,S,F,2)\n",
    "        p_ri    = tf.concat([tf.math.real(p),    tf.math.imag(p)],    axis=-1)  # (B,S,F,2)\n",
    "        # FIX: keep rank-4, do NOT expand dims\n",
    "        h_pow   = tf.abs(h) ** 2                                                   # (B,S,F,1), float32\n",
    "\n",
    "        z = tf.concat([y_ri, h_ri, s_eq_ri, h_pow, p_ri], axis=-1)  # (B,S,F,9)\n",
    "\n",
    "        # Backbone\n",
    "        z = self.input_conv(z)\n",
    "        z = self.res1(z, training=training)\n",
    "        z = self.res2(z, training=training)\n",
    "        z = self.res3(z, training=training)\n",
    "        z = self.res4(z, training=training)\n",
    "        z = self.output_conv(z)  # (B,S,F,num_bits_per_symbol)\n",
    "\n",
    "        # Drop pilot symbol (index = PILOT_SYM) so labels align\n",
    "        z_data = tf.concat([z[:, :PILOT_SYM, :, :], z[:, PILOT_SYM+1:, :, :]], axis=1)  # (B,S-1,F,bits)\n",
    "\n",
    "        # Flatten -> (B,1,1,(S-1)*F*bits)\n",
    "        z_flat = tf.reshape(z_data, [batch_size, 1, 1, -1])\n",
    "        return z_flat\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build model (unchanged I/O)\n",
    "# =========================\n",
    "input_y = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_h = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_p = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "\n",
    "receiver = NeuralReceiver()\n",
    "output_llrs = receiver([input_y, input_h, input_p])\n",
    "receiver_model = Model(inputs=[input_y, input_h, input_p], outputs=output_llrs)\n",
    "\n",
    "# =========================\n",
    "# Create pilots p_all (N,1,S,F) complex64, nonzero only at symbol PILOT_SYM\n",
    "# =========================\n",
    "p_val = tf.complex(tf.constant(1.0, tf.float32), tf.constant(0.0, tf.float32))  # 1+0j\n",
    "sym_mask = tf.one_hot(PILOT_SYM, depth=S, dtype=tf.float32)  # (S,)\n",
    "sym_mask = tf.reshape(sym_mask, [1,1,S,1])                   # (1,1,S,1)\n",
    "sc_mask = tf.ones([1,1,1,F], tf.float32)                     # (1,1,1,F) (all pilots on that symbol)\n",
    "pilot_mask = tf.cast(sym_mask * sc_mask, tf.complex64)       # (1,1,S,F)\n",
    "pilot_mask = tf.tile(pilot_mask, [N,1,1,1])                  # (N,1,S,F)\n",
    "p_all = p_val * pilot_mask                                   # (N,1,S,F) complex64\n",
    "\n",
    "# =========================\n",
    "# TRUE-BIT labels from b (data-only, pilot excluded)\n",
    "# b must have n == (S-1)*F*Bbits\n",
    "# =========================\n",
    "def true_bit_labels_from_b(b, S, F, Bbits):\n",
    "    b = tf.cast(b, tf.float32)                              # {0,1}\n",
    "    b_flat = tf.reshape(b, [tf.shape(b)[0], -1])            # (N, (S-1)*F*Bbits)\n",
    "    return tf.reshape(b_flat, [tf.shape(b_flat)[0], 1, 1, (S-1)*F*Bbits])\n",
    "\n",
    "labels_all = true_bit_labels_from_b(b, S, F, Bbits)\n",
    "print(\"labels_all shape (expect N,1,1,(S-1)*F*Bbits):\", labels_all.shape)\n",
    "\n",
    "exp_len = (S-1)*F*Bbits\n",
    "assert int(labels_all.shape[-1]) == exp_len, \\\n",
    "    f\"Label length {int(labels_all.shape[-1])} != expected {(S-1)*F*Bbits}. Check that n == (S-1)*F*Bbits at the transmitter.\"\n",
    "\n",
    "# =========================\n",
    "# Train/Val split (and align pilots with the split)\n",
    "# =========================\n",
    "y_train, y_val, h_train, h_val, l_train, l_val, train_idx, val_idx = ensure_split(\n",
    "    y_data, h_data, labels_all, batch_size=32\n",
    ")\n",
    "p_train = tf.gather(p_all, train_idx)\n",
    "p_val   = tf.gather(p_all, val_idx)\n",
    "\n",
    "# =========================\n",
    "# Compile & Train (optimizer/schedule unchanged)\n",
    "# =========================\n",
    "batch_size = 32\n",
    "total_epochs = 50\n",
    "steps_per_epoch = math.ceil(y_train.shape[0] / batch_size)\n",
    "\n",
    "warmup_epochs = 6\n",
    "decay_start_epoch = 22\n",
    "iter_limits = [\n",
    "    warmup_epochs * steps_per_epoch,\n",
    "    decay_start_epoch * steps_per_epoch,\n",
    "    total_epochs * steps_per_epoch\n",
    "]\n",
    "\n",
    "lr_schedule = CustomLRSchedule(\n",
    "    min_learning_rate=0.0,\n",
    "    max_learning_rate=1e-3,\n",
    "    iter_limits=iter_limits,\n",
    "    quad_decay=False\n",
    ")\n",
    "\n",
    "receiver_model.compile(\n",
    "    optimizer=Adam(learning_rate=lr_schedule),\n",
    "    loss=binary_sigmoid_cross_entropy\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=6, restore_best_weights=True, min_delta=1e-4\n",
    ")\n",
    "\n",
    "history = receiver_model.fit(\n",
    "    [y_train, h_train, p_train],\n",
    "    l_train,  # TRUE-BIT LABELS\n",
    "    validation_data=([y_val, h_val, p_val], l_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=total_epochs,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Classical baseline (ZF + QPSK slicer), pilot excluded\n",
    "# =========================\n",
    "def classical_qpsk_bits_from_yh(y, h, pilot_sym, tx_power=1.0, eps=1e-9):\n",
    "    \"\"\"\n",
    "    y, h: (N,1,S,F) complex64\n",
    "    Returns hard bits for data symbols only: (N, (S-1)*F*2) with QPSK Gray mapping:\n",
    "      b0 = 0 if Re>=0 else 1, b1 = 0 if Im>=0 else 1\n",
    "    \"\"\"\n",
    "    # Basic ZF equalization\n",
    "    y = tf.cast(y, tf.complex64)\n",
    "    h = tf.cast(h, tf.complex64)\n",
    "    heps = h + tf.complex(tf.constant(eps, tf.float32), tf.constant(0.0, tf.float32))\n",
    "    s_eq = y / heps\n",
    "    if tx_power != 1.0:\n",
    "        s_eq = s_eq / tf.complex(tf.constant(np.sqrt(tx_power), tf.float32), tf.constant(0.0, tf.float32))\n",
    "\n",
    "    # Drop pilot symbol along S axis (axis=2)\n",
    "    s1 = s_eq[:, :, :pilot_sym, :]      # symbols before pilot\n",
    "    s2 = s_eq[:, :, pilot_sym+1:, :]    # symbols after pilot\n",
    "    s_data = tf.concat([s1, s2], axis=2)  # (N,1,S-1,F)\n",
    "\n",
    "    # Hard QPSK bits (Gray): b0 -> Re sign, b1 -> Im sign\n",
    "    re = tf.math.real(s_data); im = tf.math.imag(s_data)\n",
    "    b0 = tf.cast(re < 0.0, tf.float32)  # 0 if >=0 else 1\n",
    "    b1 = tf.cast(im < 0.0, tf.float32)\n",
    "\n",
    "    # stack along last axis and flatten to (N, (S-1)*F*2)\n",
    "    bits = tf.stack([b0, b1], axis=-1)  # (N,1,S-1,F,2)\n",
    "    bits_flat = tf.reshape(bits, [tf.shape(bits)[0], -1])\n",
    "    return bits_flat\n",
    "\n",
    "def compute_ber(true_bits_flat, pred_bits_flat):\n",
    "    # both (N, L)\n",
    "    mism = tf.not_equal(true_bits_flat, pred_bits_flat)\n",
    "    return tf.reduce_mean(tf.cast(mism, tf.float32)).numpy()\n",
    "\n",
    "# Prepare true bits (val)\n",
    "true_bits_val = tf.reshape(l_val, [tf.shape(l_val)[0], -1])  # (N_val, (S-1)*F*2)\n",
    "\n",
    "# Classical BER\n",
    "classical_bits_val = classical_qpsk_bits_from_yh(y_val, h_val, PILOT_SYM, tx_power=TxP)\n",
    "ber_classical = compute_ber(true_bits_val, classical_bits_val)\n",
    "\n",
    "# =========================\n",
    "# Neural RX: forward + threshold sweep for BER\n",
    "# =========================\n",
    "# Forward pass -> predicted probabilities per bit\n",
    "pred_val_logits = receiver_model.predict([y_val, h_val, p_val], batch_size=batch_size, verbose=0)\n",
    "pred_val_probs  = 1.0 / (1.0 + np.exp(-pred_val_logits))  # sigmoid\n",
    "\n",
    "# Sweep thresholds to minimize BER on validation set\n",
    "def sweep_threshold_for_min_ber(true_bits, probs, thresholds):\n",
    "    best_t = None\n",
    "    best_ber = 1.0\n",
    "    for t in thresholds:\n",
    "        hard = (probs > t).astype(np.float32)\n",
    "        ber = np.mean(hard.reshape(true_bits.shape) != true_bits.numpy())\n",
    "        if ber < best_ber:\n",
    "            best_ber = ber\n",
    "            best_t = t\n",
    "    return best_t, best_ber\n",
    "\n",
    "thresholds = np.linspace(0.2, 0.8, 61)   # 0.2, 0.21, ..., 0.8\n",
    "best_t, ber_neural = sweep_threshold_for_min_ber(true_bits_val, pred_val_probs.reshape(true_bits_val.shape), thresholds)\n",
    "\n",
    "# Also report BCE of the neural logits vs true bits (no masking here since labels are all data bits)\n",
    "bce_neural = tf.reduce_mean(\n",
    "    tf.keras.losses.binary_crossentropy(true_bits_val, pred_val_probs.reshape(true_bits_val.shape))\n",
    ").numpy()\n",
    "\n",
    "# =========================\n",
    "# Print results\n",
    "# =========================\n",
    "print(f\"[VAL] Classical BER: {ber_classical:.6f}\")\n",
    "print(f\"[VAL] Neural BER @ best threshold {best_t:.3f}: {ber_neural:.6f}\")\n",
    "print(f\"[VAL] Absolute BER improvement: {ber_classical - ber_neural:+.6f}\")\n",
    "rel_impr = 100.0 * (ber_classical - ber_neural) / max(ber_classical, 1e-12)\n",
    "print(f\"[VAL] Relative BER improvement: {rel_impr:+.2f}%\")\n",
    "print(f\"[VAL] Neural BCE vs true bits: {bce_neural:.6f}\")\n",
    "\n",
    "# Optional: show what BER would be at the conventional 0.5 threshold\n",
    "hard05 = (pred_val_probs.reshape(true_bits_val.shape) > 0.5).astype(np.float32)\n",
    "ber_neural_05 = np.mean(hard05 != true_bits_val.numpy())\n",
    "print(f\"[VAL] Neural BER @ threshold 0.500: {ber_neural_05:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2dae8f31-19cf-4860-ba90-7ac7c53fed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Classical BCE vs true bits: 0.519319\n",
      "[VAL] Neural     BCE vs true bits: 0.080140\n",
      "\n",
      "[VAL] Classical BER @ thr 0.500: 0.032360\n",
      "[VAL] Neural     BER @ best thr 0.510: 0.038430\n",
      "[VAL] Neural     BER @ thr 0.500: 0.038442\n",
      "[VAL] Absolute BER improvement (classical - neural_best): -0.006070\n",
      "[VAL] Relative BER improvement: -18.76%\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Classical vs Neural evaluation + threshold sweep (single cell)\n",
    "# =========================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ---- helpers ----\n",
    "def as_flat_data_only_grid(arr, S, F, Bbits, pilot_sym):\n",
    "    \"\"\"\n",
    "    Coerce incoming array of LLRs or labels into shape (N,1,1,(S-1)*F*Bbits),\n",
    "    removing the pilot symbol along axis=1 (symbol index).\n",
    "    Accepts shapes:\n",
    "      - (N, 1, 1, (S-1)*F*Bbits)  -> returned as-is\n",
    "      - (N, S, F, Bbits)          -> drop pilot sym -> flatten\n",
    "      - (N, 1, 1,  S*F*Bbits)     -> reshape -> drop pilot -> flatten\n",
    "    \"\"\"\n",
    "    arr = tf.convert_to_tensor(arr)\n",
    "    shp = arr.shape\n",
    "    N = tf.shape(arr)[0]\n",
    "    # already correct\n",
    "    if len(shp) == 4 and int(shp[1]) == 1 and int(shp[2]) == 1 and int(shp[3]) == (S-1)*F*Bbits:\n",
    "        return arr\n",
    "\n",
    "    # grid form (N,S,F,Bbits)\n",
    "    if len(shp) == 4 and int(shp[1]) == S and int(shp[2]) == F and int(shp[3]) == Bbits:\n",
    "        grid = arr\n",
    "    # flat full including pilot: (N,1,1,S*F*Bbits)\n",
    "    elif len(shp) == 4 and int(shp[1]) == 1 and int(shp[2]) == 1 and int(shp[3]) == S*F*Bbits:\n",
    "        grid = tf.reshape(arr, [N, S, F, Bbits])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input shape for as_flat_data_only_grid: {shp}\")\n",
    "\n",
    "    # drop pilot symbol (index = pilot_sym)\n",
    "    grid_data = tf.concat([grid[:, :pilot_sym, :, :], grid[:, pilot_sym+1:, :, :]], axis=1)  # (N,S-1,F,Bbits)\n",
    "    flat = tf.reshape(grid_data, [N, 1, 1, (S-1)*F*Bbits])\n",
    "    return flat\n",
    "\n",
    "def bce_from_probs(true_bits_flat, probs_flat):\n",
    "    \"\"\"Binary cross-entropy averaged over valid bits (supports -1 masking in true bits).\"\"\"\n",
    "    true_bits = tf.cast(true_bits_flat, tf.float32)\n",
    "    probs = tf.cast(probs_flat, tf.float32)\n",
    "    valid = tf.not_equal(true_bits, -1.0)\n",
    "    t = tf.boolean_mask(true_bits, valid)\n",
    "    p = tf.boolean_mask(probs, valid)\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(t, p))\n",
    "\n",
    "def ber_from_probs(true_bits_flat, probs_flat, thr=0.5):\n",
    "    \"\"\"Bit error rate with threshold on probabilities; supports -1 masking in labels.\"\"\"\n",
    "    true_bits = tf.cast(true_bits_flat, tf.float32)\n",
    "    probs = tf.cast(probs_flat, tf.float32)\n",
    "    valid = tf.not_equal(true_bits, -1.0)\n",
    "    t = tf.boolean_mask(true_bits, valid)\n",
    "    p = tf.boolean_mask(probs, valid)\n",
    "    hard = tf.cast(p > thr, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.not_equal(hard, t), tf.float32))\n",
    "\n",
    "def sigmoid_from_llr(llr_flat):\n",
    "    return tf.sigmoid(tf.cast(llr_flat, tf.float32))\n",
    "\n",
    "def try_get_val_indices():\n",
    "    g = globals()\n",
    "    if 'val_idx' in g:\n",
    "        return g['val_idx']\n",
    "    if 'val_indices' in g:\n",
    "        return g['val_indices']\n",
    "    # Not critical if we don’t have it; only needed if we slice llr_probs\n",
    "    return None\n",
    "\n",
    "# --- optional fallback classical demapper if llr_probs is missing ---\n",
    "def classical_probs_fallback_zf_qpsk(y_val, h_val, S, F, Bbits, PILOT_SYM):\n",
    "    \"\"\"\n",
    "    Very simple fallback:\n",
    "      - Perfect-CSI equalization: s_hat = y / h per RE\n",
    "      - Drop pilot symbol\n",
    "      - Assume QPSK Gray and build bit probabilities from signs (hard -> probs {0,1})\n",
    "    Only used when llr_probs is not available. If Bbits != 2, this is not a fair baseline.\n",
    "    \"\"\"\n",
    "    if Bbits != 2:\n",
    "        raise RuntimeError(\"Fallback classical only supports QPSK (num_bits_per_symbol==2). Provide llr_probs for a fair baseline.\")\n",
    "    # Shapes (N,1,S,F) complex\n",
    "    y = tf.transpose(y_val, [0, 2, 3, 1])  # (N,S,F,1)\n",
    "    h = tf.transpose(h_val, [0, 2, 3, 1])\n",
    "    # Avoid divide-by-zero\n",
    "    eps = tf.constant(1e-6, tf.float32)\n",
    "    s_hat = y / (h + tf.cast(eps, h.dtype))\n",
    "    # drop pilot sym\n",
    "    s_hat = tf.concat([s_hat[:, :PILOT_SYM, :, :], s_hat[:, PILOT_SYM+1:, :, :]], axis=1)  # (N,S-1,F,1)\n",
    "    s_hat = tf.squeeze(s_hat, axis=-1)  # (N,S-1,F)\n",
    "\n",
    "    # Hard bits: Gray-QPSK: b0 from I sign, b1 from Q sign (00 at +,+)\n",
    "    I = tf.math.real(s_hat)\n",
    "    Q = tf.math.imag(s_hat)\n",
    "    b0 = tf.cast(I < 0.0, tf.float32)  # I>=0 -> 0, I<0 -> 1\n",
    "    b1 = tf.cast(Q < 0.0, tf.float32)  # Q>=0 -> 0, Q<0 -> 1\n",
    "    # interleave bits per symbol to (N,S-1,F,Bbits)\n",
    "    bits = tf.stack([b0, b1], axis=-1)\n",
    "    probs = bits  # {0,1} \"probabilities\" as hard decisions\n",
    "    probs_flat = tf.reshape(bits, [tf.shape(bits)[0], 1, 1, (S-1)*F*Bbits])\n",
    "    return probs_flat\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Neural probabilities on VAL\n",
    "# ----------------------------\n",
    "pred_val_llr = receiver_model.predict([y_val, h_val, p_val], verbose=0)           # (Nv,1,1,(S-1)*F*Bbits)\n",
    "neural_probs = tf.sigmoid(tf.convert_to_tensor(pred_val_llr, dtype=tf.float32))   # probs\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Classical probabilities on VAL\n",
    "#    Preferred: from llr_probs (already your classical demapper)\n",
    "#    Fallback: ZF+QPSK hard-slicer\n",
    "# ----------------------------\n",
    "use_fallback = False\n",
    "if 'llr_probs' in globals():\n",
    "    try:\n",
    "        # Slice to validation set if we have indices\n",
    "        vidx = try_get_val_indices()\n",
    "        llr_all = globals()['llr_probs']\n",
    "        if vidx is not None:\n",
    "            llr_all = tf.gather(llr_all, vidx)\n",
    "        classical_llr_val = as_flat_data_only_grid(llr_all, S, F, Bbits, PILOT_SYM)  # (Nv,1,1,(S-1)*F*Bbits)\n",
    "        classical_probs = sigmoid_from_llr(classical_llr_val)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not use provided llr_probs ({e}). Falling back to ZF+QPSK hard-slicer.\")\n",
    "        use_fallback = True\n",
    "else:\n",
    "    use_fallback = True\n",
    "\n",
    "if use_fallback:\n",
    "    classical_probs = classical_probs_fallback_zf_qpsk(y_val, h_val, S, F, Bbits, PILOT_SYM)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Metrics vs TRUE bits (l_val)\n",
    "# ----------------------------\n",
    "# BCE (lower is better)\n",
    "bce_neural     = float(bce_from_probs(l_val, neural_probs))\n",
    "bce_classical  = float(bce_from_probs(l_val, classical_probs))\n",
    "\n",
    "# BER at default 0.5\n",
    "ber_neural_050    = float(ber_from_probs(l_val, neural_probs, thr=0.5))\n",
    "ber_classical_050 = float(ber_from_probs(l_val, classical_probs, thr=0.5))\n",
    "\n",
    "# Sweep thresholds for neural to get best BER\n",
    "thr_grid = np.linspace(0.3, 0.7, 41)  # dense enough; adjust if needed\n",
    "ber_sweep = [float(ber_from_probs(l_val, neural_probs, thr=float(t))) for t in thr_grid]\n",
    "best_idx = int(np.argmin(ber_sweep))\n",
    "best_thr = float(thr_grid[best_idx])\n",
    "best_ber = float(ber_sweep[best_idx])\n",
    "\n",
    "# Improvements\n",
    "abs_impr = ber_classical_050 - best_ber\n",
    "rel_impr = 100.0 * abs_impr / max(ber_classical_050, 1e-12)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Print report\n",
    "# ----------------------------\n",
    "print(f\"[VAL] Classical BCE vs true bits: {bce_classical:.6f}\")\n",
    "print(f\"[VAL] Neural     BCE vs true bits: {bce_neural:.6f}\")\n",
    "print()\n",
    "print(f\"[VAL] Classical BER @ thr 0.500: {ber_classical_050:.6f}\")\n",
    "print(f\"[VAL] Neural     BER @ best thr {best_thr:.3f}: {best_ber:.6f}\")\n",
    "print(f\"[VAL] Neural     BER @ thr 0.500: {ber_neural_050:.6f}\")\n",
    "print(f\"[VAL] Absolute BER improvement (classical - neural_best): {abs_impr:+.6f}\")\n",
    "print(f\"[VAL] Relative BER improvement: {rel_impr:+.2f}%\")\n",
    "if use_fallback:\n",
    "    print(\"\\n[NOTE] Classical baseline used fallback ZF+QPSK hard-slicer (no llr_probs available). \"\n",
    "          \"For a fairer comparison on higher-order QAM, provide llr_probs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc42c94-0be2-48e3-82f3-62a036815404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NI's Version of DeepRX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f3ba4fe-ce3e-4b24-acb0-dbdaeb479713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlockProperties:\n",
    "    \"\"\"Structure that holds all configurational parameters of the ResNet blocks.\"\"\"\n",
    "\n",
    "    num_blocks: int = 0\n",
    "    kernel_size: list = []\n",
    "    dilation_rate: list = []\n",
    "    num_filter: list = []\n",
    "\n",
    "class DeepRx:\n",
    "    \"\"\"DeepRx neural network model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_ofdm_sym: int,\n",
    "        num_subcar: int,\n",
    "        num_ant: int,\n",
    "        res_net_config: ResNetBlockProperties,\n",
    "        num_output_llr: int,\n",
    "        use_submodels: bool,\n",
    "    ):\n",
    "        \"\"\"Initializes the network topology\"\"\"\n",
    "\n",
    "        # number of symbols in time: S\n",
    "        self.num_ofdm_sym = num_ofdm_sym\n",
    "        # number of subcarriers: F\n",
    "        self.num_subcar = num_subcar\n",
    "        # number of antennas: N_r\n",
    "        self.num_ant = num_ant\n",
    "        # number of output LLRs: B\n",
    "        self.num_output_llr = num_output_llr\n",
    "\n",
    "        # RX data input: complex value already split into two channels\n",
    "        y = tf.keras.layers.Input(shape=(num_ofdm_sym, num_subcar, 2 * num_ant), name=\"RX-Data-In\")\n",
    "        # TX pilots: complex value already split into two channels\n",
    "        x_p = tf.keras.layers.Input(shape=(num_ofdm_sym, num_subcar, 2), name=\"TX-Pilot-In\")\n",
    "        # raw channel estimate: complex value already split into two channels\n",
    "        h_r = tf.keras.layers.Input(shape=(num_ofdm_sym, num_subcar, 2 * num_ant), name=\"Raw-Channel-Est-In\")\n",
    "        # concatenate input layers\n",
    "        concat = tf.keras.layers.concatenate([y, x_p, h_r], name=\"Concat\")\n",
    "        # convolutional input layer\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), dilation_rate=(1, 1), padding=\"same\", activation=None, name=\"Conv-In\")(\n",
    "            concat\n",
    "        )\n",
    "        # construct ResNet blocks\n",
    "        for block_idx in range(res_net_config.num_blocks):\n",
    "            # generate ResNet block as compact sub-models\n",
    "            if use_submodels:\n",
    "                _, res_net_block = self.create_res_net_block(\n",
    "                    input=x,\n",
    "                    filter_size=tuple(res_net_config.kernel_size[block_idx]),\n",
    "                    dilation=tuple(res_net_config.dilation_rate[block_idx]),\n",
    "                    num_filter=res_net_config.num_filter[block_idx],\n",
    "                    res_net_block_idx=block_idx,\n",
    "                )\n",
    "                x = res_net_block(x)\n",
    "            # generate ResNet block with all sub-layers visible\n",
    "            else:\n",
    "                x, _ = self.create_res_net_block(\n",
    "                    input=x,\n",
    "                    filter_size=tuple(res_net_config.kernel_size[block_idx]),\n",
    "                    dilation=tuple(res_net_config.dilation_rate[block_idx]),\n",
    "                    num_filter=res_net_config.num_filter[block_idx],\n",
    "                    res_net_block_idx=block_idx,\n",
    "                )\n",
    "        # convolutional output layer: LLR\n",
    "        output = tf.keras.layers.Conv2D(\n",
    "            self.num_output_llr, (3, 3), dilation_rate=(1, 1), padding=\"same\", activation=None, name=\"Conv-Out\"\n",
    "        )(x)\n",
    "        # instantiate complete model\n",
    "        self.model = tf.keras.Model(inputs=[y, x_p, h_r], outputs=output, name=\"DeepRx\")\n",
    "\n",
    "    def create_res_net_block(\n",
    "        self,\n",
    "        input: tf.Tensor,\n",
    "        filter_size: tuple,\n",
    "        dilation: tuple,\n",
    "        num_filter: int,\n",
    "        res_net_block_idx: int,\n",
    "    ):\n",
    "        \"\"\"Creates the ResNet sub-blocks\"\"\"\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"BN-{res_net_block_idx}-0\")(input)\n",
    "        x = tf.keras.layers.ReLU(name=f\"ReLU-{res_net_block_idx}-0\")(x)\n",
    "        x = tf.keras.layers.SeparableConv2D(\n",
    "            filters=num_filter,\n",
    "            kernel_size=filter_size,\n",
    "            dilation_rate=dilation,\n",
    "            depth_multiplier=1,\n",
    "            padding=\"same\",\n",
    "            name=f\"Separable-Conv-{res_net_block_idx}-0\",\n",
    "        )(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"BN-{res_net_block_idx}-1\")(x)\n",
    "        x = tf.keras.layers.ReLU(name=f\"ReLU-{res_net_block_idx}-1\")(x)\n",
    "        x = tf.keras.layers.SeparableConv2D(\n",
    "            filters=num_filter,\n",
    "            kernel_size=filter_size,\n",
    "            dilation_rate=dilation,\n",
    "            depth_multiplier=1,\n",
    "            padding=\"same\",\n",
    "            name=f\"Separable-Conv-{res_net_block_idx}-1\",\n",
    "        )(x)\n",
    "\n",
    "        # When ResNet block's output depth is increased or decreased, also the residual path has to be up- or downsampled.\n",
    "        # This can be achieved via 1x1 convolutions.\n",
    "        if input.shape[3] != num_filter:\n",
    "            x_shortcut = tf.keras.layers.Conv2D(num_filter, (1, 1), name=f\"Conv-Depth-Adjust-{res_net_block_idx}\")(\n",
    "                input\n",
    "            )\n",
    "            # TODO: do we need additional BN and ReLU activation here?\n",
    "            x_shortcut = tf.keras.layers.BatchNormalization(name=f\"BN-{res_net_block_idx}-2\")(x_shortcut)\n",
    "            # x_shortcut = tf.keras.layers.ReLU(name=f\"ReLU-{res_net_block_idx}-2\")(x_shortcut)\n",
    "        else:\n",
    "            x_shortcut = input\n",
    "\n",
    "        output = tf.keras.layers.Add(name=f\"Add-{res_net_block_idx}\")([x_shortcut, x])\n",
    "\n",
    "        res_net_model = tf.keras.Model(inputs=input, outputs=output, name=f\"ResNet-Block-{res_net_block_idx}\")\n",
    "\n",
    "        return output, res_net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39b30d4f-9be5-400e-abdd-b834e2dd59ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepRx\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " RX-Data-In (InputLayer)     [(None, 14, 96, 2)]          0         []                            \n",
      "                                                                                                  \n",
      " TX-Pilot-In (InputLayer)    [(None, 14, 96, 2)]          0         []                            \n",
      "                                                                                                  \n",
      " Raw-Channel-Est-In (InputL  [(None, 14, 96, 2)]          0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " Concat (Concatenate)        (None, 14, 96, 6)            0         ['RX-Data-In[0][0]',          \n",
      "                                                                     'TX-Pilot-In[0][0]',         \n",
      "                                                                     'Raw-Channel-Est-In[0][0]']  \n",
      "                                                                                                  \n",
      " Conv-In (Conv2D)            (None, 14, 96, 64)           3520      ['Concat[0][0]']              \n",
      "                                                                                                  \n",
      " BN-0-0 (BatchNormalization  (None, 14, 96, 64)           256       ['Conv-In[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-0-0 (ReLU)             (None, 14, 96, 64)           0         ['BN-0-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-0-0 (Separa  (None, 14, 96, 64)           4736      ['ReLU-0-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-0-1 (BatchNormalization  (None, 14, 96, 64)           256       ['Separable-Conv-0-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-0-1 (ReLU)             (None, 14, 96, 64)           0         ['BN-0-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-0-1 (Separa  (None, 14, 96, 64)           4736      ['ReLU-0-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-0 (Add)                 (None, 14, 96, 64)           0         ['Conv-In[0][0]',             \n",
      "                                                                     'Separable-Conv-0-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-1-0 (BatchNormalization  (None, 14, 96, 64)           256       ['Add-0[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-1-0 (ReLU)             (None, 14, 96, 64)           0         ['BN-1-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-1-0 (Separa  (None, 14, 96, 64)           4736      ['ReLU-1-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-1-1 (BatchNormalization  (None, 14, 96, 64)           256       ['Separable-Conv-1-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-1-1 (ReLU)             (None, 14, 96, 64)           0         ['BN-1-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-1-1 (Separa  (None, 14, 96, 64)           4736      ['ReLU-1-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-1 (Add)                 (None, 14, 96, 64)           0         ['Add-0[0][0]',               \n",
      "                                                                     'Separable-Conv-1-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-2-0 (BatchNormalization  (None, 14, 96, 64)           256       ['Add-1[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-2-0 (ReLU)             (None, 14, 96, 64)           0         ['BN-2-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-2-0 (Separa  (None, 14, 96, 96)           6816      ['ReLU-2-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-2-1 (BatchNormalization  (None, 14, 96, 96)           384       ['Separable-Conv-2-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Conv-Depth-Adjust-2 (Conv2  (None, 14, 96, 96)           6240      ['Add-1[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " ReLU-2-1 (ReLU)             (None, 14, 96, 96)           0         ['BN-2-1[0][0]']              \n",
      "                                                                                                  \n",
      " BN-2-2 (BatchNormalization  (None, 14, 96, 96)           384       ['Conv-Depth-Adjust-2[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Separable-Conv-2-1 (Separa  (None, 14, 96, 96)           10176     ['ReLU-2-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-2 (Add)                 (None, 14, 96, 96)           0         ['BN-2-2[0][0]',              \n",
      "                                                                     'Separable-Conv-2-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-3-0 (BatchNormalization  (None, 14, 96, 96)           384       ['Add-2[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-3-0 (ReLU)             (None, 14, 96, 96)           0         ['BN-3-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-3-0 (Separa  (None, 14, 96, 96)           10176     ['ReLU-3-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-3-1 (BatchNormalization  (None, 14, 96, 96)           384       ['Separable-Conv-3-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-3-1 (ReLU)             (None, 14, 96, 96)           0         ['BN-3-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-3-1 (Separa  (None, 14, 96, 96)           10176     ['ReLU-3-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-3 (Add)                 (None, 14, 96, 96)           0         ['Add-2[0][0]',               \n",
      "                                                                     'Separable-Conv-3-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-4-0 (BatchNormalization  (None, 14, 96, 96)           384       ['Add-3[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-4-0 (ReLU)             (None, 14, 96, 96)           0         ['BN-4-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-4-0 (Separa  (None, 14, 96, 128)          13280     ['ReLU-4-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-4-1 (BatchNormalization  (None, 14, 96, 128)          512       ['Separable-Conv-4-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Conv-Depth-Adjust-4 (Conv2  (None, 14, 96, 128)          12416     ['Add-3[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " ReLU-4-1 (ReLU)             (None, 14, 96, 128)          0         ['BN-4-1[0][0]']              \n",
      "                                                                                                  \n",
      " BN-4-2 (BatchNormalization  (None, 14, 96, 128)          512       ['Conv-Depth-Adjust-4[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Separable-Conv-4-1 (Separa  (None, 14, 96, 128)          17664     ['ReLU-4-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-4 (Add)                 (None, 14, 96, 128)          0         ['BN-4-2[0][0]',              \n",
      "                                                                     'Separable-Conv-4-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-5-0 (BatchNormalization  (None, 14, 96, 128)          512       ['Add-4[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-5-0 (ReLU)             (None, 14, 96, 128)          0         ['BN-5-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-5-0 (Separa  (None, 14, 96, 128)          17664     ['ReLU-5-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-5-1 (BatchNormalization  (None, 14, 96, 128)          512       ['Separable-Conv-5-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-5-1 (ReLU)             (None, 14, 96, 128)          0         ['BN-5-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-5-1 (Separa  (None, 14, 96, 128)          17664     ['ReLU-5-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-5 (Add)                 (None, 14, 96, 128)          0         ['Add-4[0][0]',               \n",
      "                                                                     'Separable-Conv-5-1[0][0]']  \n",
      "                                                                                                  \n",
      " Conv-Out (Conv2D)           (None, 14, 96, 2)            2306      ['Add-5[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 152290 (594.88 KB)\n",
      "Trainable params: 149666 (584.63 KB)\n",
      "Non-trainable params: 2624 (10.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deeprx = DeepRx(num_ofdm_sym=num_symbols,\n",
    "                num_subcar=num_SC,\n",
    "                num_ant=1,\n",
    "                res_net_config=cfg,\n",
    "                num_output_llr=num_bits_per_symbol,\n",
    "                use_submodels=False).model\n",
    "\n",
    "deeprx.summary()  # Verify output shape is (None, S, F, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d6085ca-e24c-4632-a45f-edc1f4c85f72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m llr_bits_full \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(llr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     56\u001b[0m llr_bits_full \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(llr_bits_full, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_symbols, num_SC, num_bits_per_symbol])\n\u001b[0;32m---> 58\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m \u001b[43mindices\u001b[49m[:train_size]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ---- Train/Val split ----\u001b[39;00m\n\u001b[1;32m     61\u001b[0m y_train   \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(y_in,  train_indices)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indices' is not defined"
     ]
    }
   ],
   "source": [
    "# === DeepRx: Train only (stable losses, no evaluation) ===\n",
    "\n",
    "# ---- Helpers ----\n",
    "def complex_to_2ch(x):\n",
    "    return tf.concat([tf.math.real(x), tf.math.imag(x)], axis=-1)\n",
    "\n",
    "def norm_per_sample(t):\n",
    "    # Normalize over (S,F,C) per sample for stability\n",
    "    mean = tf.reduce_mean(t, axis=[1,2,3], keepdims=True)\n",
    "    std  = tf.math.reduce_std(t, axis=[1,2,3], keepdims=True) + 1e-6\n",
    "    return (t - mean) / std\n",
    "\n",
    "# ---- ResNet config (adjust as you like) ----\n",
    "cfg = ResNetBlockProperties()\n",
    "cfg.num_blocks = 6\n",
    "cfg.kernel_size = [[3, 3]] * 6\n",
    "cfg.dilation_rate = [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4)]\n",
    "cfg.num_filter = [64, 64, 96, 96, 128, 128]\n",
    "\n",
    "# ---- Build model (your classes already defined above) ----\n",
    "deep_rx_model = DeepRx(\n",
    "    num_ofdm_sym=num_symbols,          # S\n",
    "    num_subcar=num_SC,                 # F\n",
    "    num_ant=1,                         # set to your N_r if >1 and adjust inputs accordingly\n",
    "    res_net_config=cfg,\n",
    "    num_output_llr=num_bits_per_symbol,# B\n",
    "    use_submodels=False\n",
    ").model\n",
    "\n",
    "deep_rx_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "# ---- Prepare inputs (NHWC) ----\n",
    "# y_data: (N, 1, S, F) complex64 -> (N, S, F, 2)\n",
    "y_data = tf.squeeze(y, axis=(2)) \n",
    "y_in  = tf.squeeze(y_data, axis=1)\n",
    "y_in  = tf.expand_dims(y_in, -1)\n",
    "y_in  = complex_to_2ch(y_in)\n",
    "\n",
    "# h_data: ensure -> (N, S, F, 2)\n",
    "h_data = tf.squeeze(h_freq, axis=[1,2,3])\n",
    "if len(h_data.shape) == 4 and h_data.shape[1] == 1:\n",
    "    h_core = tf.squeeze(h_data, axis=1)\n",
    "else:\n",
    "    h_core = h_data\n",
    "h_in  = complex_to_2ch(tf.expand_dims(h_core, -1))\n",
    "\n",
    "# pilots: zeros if no pilot grid available\n",
    "x_p_in = tf.zeros((tf.shape(y_in)[0], num_symbols, num_SC, 2), dtype=tf.float32)\n",
    "\n",
    "# Labels: HARD bits shaped to (N, S, F, B)\n",
    "llr_bits_full = tf.cast(llr > 0.0, tf.float32)\n",
    "llr_bits_full = tf.reshape(llr_bits_full, [-1, num_symbols, num_SC, num_bits_per_symbol])\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "\n",
    "# ---- Train/Val split ----\n",
    "y_train   = tf.gather(y_in,  train_indices)\n",
    "y_val_in  = tf.gather(y_in,  val_indices)\n",
    "h_train   = tf.gather(h_in,  train_indices)\n",
    "h_val_in  = tf.gather(h_in,  val_indices)\n",
    "x_p_train = tf.gather(x_p_in, train_indices)\n",
    "x_p_val   = tf.gather(x_p_in, val_indices)\n",
    "lbl_train = tf.gather(llr_bits_full, train_indices)\n",
    "lbl_val   = tf.gather(llr_bits_full, val_indices)\n",
    "\n",
    "# ---- Optional: light normalization for stability (keep pilots as-is) ----\n",
    "y_train_n, y_val_n = norm_per_sample(y_train), norm_per_sample(y_val_in)\n",
    "h_train_n, h_val_n = norm_per_sample(h_train), norm_per_sample(h_val_in)\n",
    "x_p_train_n, x_p_val_n = x_p_train, x_p_val\n",
    "\n",
    "# ---- Sanity prints ----\n",
    "print(\"Inputs:\",\n",
    "      \"\\ny_train:\", y_train_n.shape,\n",
    "      \"\\nh_train:\", h_train_n.shape,\n",
    "      \"\\nx_p_train:\", x_p_train_n.shape,\n",
    "      \"\\nlabels:\", lbl_train.shape)\n",
    "\n",
    "# ---- Callbacks for stable training ----\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-5),\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('deeprx_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ---- Train (no evaluation here) ----\n",
    "history = deep_rx_model.fit(\n",
    "    [y_train_n, x_p_train_n, h_train_n], lbl_train,\n",
    "    validation_data=([y_val_n, x_p_val_n, h_val_n], lbl_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e316950-04e6-4122-87a2-df4a3be80635",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNetBlockProperties' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ---- ResNet config (adjust as you like) ----\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mResNetBlockProperties\u001b[49m()\n\u001b[1;32m     15\u001b[0m cfg\u001b[38;5;241m.\u001b[39mnum_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     16\u001b[0m cfg\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNetBlockProperties' is not defined"
     ]
    }
   ],
   "source": [
    "# === DeepRx: Train only (using GROUND-TRUTH bits as labels; no evaluation) ===\n",
    "\n",
    "# ---- Helpers ----\n",
    "def complex_to_2ch(x):\n",
    "    return tf.concat([tf.math.real(x), tf.math.imag(x)], axis=-1)\n",
    "\n",
    "def norm_per_sample(t):\n",
    "    # Normalize over (S,F,C) per sample for stability\n",
    "    mean = tf.reduce_mean(t, axis=[1,2,3], keepdims=True)\n",
    "    std  = tf.math.reduce_std(t, axis=[1,2,3], keepdims=True) + 1e-6\n",
    "    return (t - mean) / std\n",
    "\n",
    "# ---- ResNet config (adjust as you like) ----\n",
    "cfg = ResNetBlockProperties()\n",
    "cfg.num_blocks = 6\n",
    "cfg.kernel_size = [[3, 3]] * 6\n",
    "cfg.dilation_rate = [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4)]\n",
    "cfg.num_filter = [64, 64, 96, 96, 128, 128]\n",
    "\n",
    "# ---- Build model (your classes already defined above) ----\n",
    "deep_rx_model = DeepRx(\n",
    "    num_ofdm_sym=num_symbols,          # S\n",
    "    num_subcar=num_SC,                 # F\n",
    "    num_ant=1,                         # set to your N_r if >1 and adjust inputs accordingly\n",
    "    res_net_config=cfg,\n",
    "    num_output_llr=num_bits_per_symbol,# B\n",
    "    use_submodels=False\n",
    ").model\n",
    "\n",
    "deep_rx_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "# ---- Prepare inputs (NHWC) ----\n",
    "# y_data: (N, 1, S, F) complex64 -> (N, S, F, 2)\n",
    "y_in  = tf.squeeze(y_data, axis=1)\n",
    "y_in  = tf.expand_dims(y_in, -1)\n",
    "y_in  = complex_to_2ch(y_in)\n",
    "\n",
    "# h_data: ensure -> (N, S, F, 2)\n",
    "if len(h_data.shape) == 4 and h_data.shape[1] == 1:\n",
    "    h_core = tf.squeeze(h_data, axis=1)\n",
    "else:\n",
    "    h_core = h_data\n",
    "h_in  = complex_to_2ch(tf.expand_dims(h_core, -1))\n",
    "\n",
    "# pilots: zeros if no pilot grid available\n",
    "x_p_in = tf.zeros((tf.shape(y_in)[0], num_symbols, num_SC, 2), dtype=tf.float32)\n",
    "\n",
    "# ---- Labels: GROUND-TRUTH bits shaped to (N, S, F, B) ----\n",
    "# b: (N, num_ut, rg.num_streams_per_tx, S*F*B)\n",
    "b_all_flat = tf.squeeze(b, axis=[1, 2])  # -> (N, S*F*B)\n",
    "gt_bits = tf.reshape(tf.cast(b_all_flat, tf.float32),\n",
    "                     [-1, num_symbols, num_SC, num_bits_per_symbol])  # -> (N, S, F, B)\n",
    "\n",
    "# ---- Train/Val split ----\n",
    "y_train   = tf.gather(y_in,  train_indices)\n",
    "y_val_in  = tf.gather(y_in,  val_indices)\n",
    "h_train   = tf.gather(h_in,  train_indices)\n",
    "h_val_in  = tf.gather(h_in,  val_indices)\n",
    "x_p_train = tf.gather(x_p_in, train_indices)\n",
    "x_p_val   = tf.gather(x_p_in, val_indices)\n",
    "lbl_train = tf.gather(gt_bits, train_indices)   # (trainN, S, F, B)\n",
    "lbl_val   = tf.gather(gt_bits, val_indices)     # (valN,   S, F, B)\n",
    "\n",
    "# ---- Optional: light normalization for stability (keep pilots as-is) ----\n",
    "y_train_n, y_val_n = norm_per_sample(y_train), norm_per_sample(y_val_in)\n",
    "h_train_n, h_val_n = norm_per_sample(h_train), norm_per_sample(h_val_in)\n",
    "x_p_train_n, x_p_val_n = x_p_train, x_p_val\n",
    "\n",
    "# ---- Sanity prints ----\n",
    "print(\"Inputs:\",\n",
    "      \"\\ny_train:\", y_train_n.shape,\n",
    "      \"\\nh_train:\", h_train_n.shape,\n",
    "      \"\\nx_p_train:\", x_p_train_n.shape,\n",
    "      \"\\nlabels (GT):\", lbl_train.shape)\n",
    "\n",
    "# ---- Callbacks for stable training ----\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-5),\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('deeprx_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ---- Train (no evaluation here) ----\n",
    "history = deep_rx_model.fit(\n",
    "    [y_train_n, x_p_train_n, h_train_n], lbl_train,\n",
    "    validation_data=([y_val_n, x_p_val_n, h_val_n], lbl_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c8ebe7a-5977-425d-bf25-3bb5d554d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best weights from 'deeprx_best.h5'.\n",
      "Neural Rx (DeepRx) BER [validation]: 0.054094\n",
      "Classical ZF BER [validation]: 0.035301\n",
      "Relative BER change (Neural vs ZF): +53.24%\n"
     ]
    }
   ],
   "source": [
    "# === BER Benchmark: Neural Rx (DeepRx) vs Classical/ZF on validation split ===\n",
    "\n",
    "import os\n",
    "\n",
    "# 1) (Optional) load best checkpoint before evaluating\n",
    "try:\n",
    "    if os.path.exists('deeprx_best.h5'):\n",
    "        deep_rx_model.load_weights('deeprx_best.h5')\n",
    "        print(\"Loaded best weights from 'deeprx_best.h5'.\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping weight load:\", e)\n",
    "\n",
    "# 2) Predict LLR logits with DeepRx and hard-decode to bits\n",
    "llr_pred_neural = deep_rx_model.predict([y_val_n, x_p_val_n, h_val_n], batch_size=32, verbose=0)  # (valN, S, F, B)\n",
    "bits_neural = tf.cast(llr_pred_neural > 0.0, tf.float32)                                         # (valN, S, F, B)\n",
    "bits_neural_flat = tf.reshape(bits_neural, [tf.shape(bits_neural)[0], -1])                        # (valN, S*F*B)\n",
    "\n",
    "# 3) Ground-truth bits matching the same validation subset\n",
    "#    b: (N, num_ut, rg.num_streams_per_tx, S*F*B) from your Tx pipeline\n",
    "b_all_flat = tf.squeeze(b, axis=[1, 2])                                   # (N, S*F*B)\n",
    "b_val_flat = tf.cast(tf.gather(b_all_flat, val_indices), tf.float32)      # (valN, S*F*B)\n",
    "\n",
    "# Sanity check: shapes must match for BER\n",
    "assert int(b_val_flat.shape[1]) == int(bits_neural_flat.shape[1]), \\\n",
    "    f\"GT bits and Neural bits mismatch: {b_val_flat.shape[1]} vs {bits_neural_flat.shape[1]}\"\n",
    "\n",
    "# 4) Neural Rx BER\n",
    "ber_neural = compute_ber(b_val_flat, bits_neural_flat).numpy()\n",
    "print(f\"Neural Rx (DeepRx) BER [validation]: {ber_neural:.6f}\")\n",
    "\n",
    "# 5) Classical/ZF baseline BER on the SAME validation split\n",
    "#    llr: (N, S, F, B) from your earlier ZF demapper (before training)\n",
    "try:\n",
    "    bits_zf = tf.cast(llr > 0.0, tf.float32)                               # (N, S, F, B)\n",
    "    bits_zf_flat = tf.reshape(bits_zf, [tf.shape(bits_zf)[0], -1])         # (N, S*F*B)\n",
    "    bits_zf_val_flat = tf.gather(bits_zf_flat, val_indices)                # (valN, S*F*B)\n",
    "\n",
    "    # Sanity check\n",
    "    assert int(b_val_flat.shape[1]) == int(bits_zf_val_flat.shape[1]), \\\n",
    "        f\"GT bits and ZF bits mismatch: {b_val_flat.shape[1]} vs {bits_zf_val_flat.shape[1]}\"\n",
    "\n",
    "    ber_zf = compute_ber(b_val_flat, bits_zf_val_flat).numpy()\n",
    "    print(f\"Classical ZF BER [validation]: {ber_zf:.6f}\")\n",
    "\n",
    "    # 6) Relative comparison\n",
    "    if ber_zf > 0:\n",
    "        rel = (ber_neural - ber_zf) / ber_zf\n",
    "        print(f\"Relative BER change (Neural vs ZF): {rel:+.2%}\")\n",
    "except Exception as e:\n",
    "    print(\"ZF baseline BER skipped (missing or incompatible 'llr'):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be593b-803f-43d9-ab72-b35bec90ff55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
