{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bcb26b8-3f1e-45e8-b2f1-fef291f27c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 15:24:23.289894: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-04 15:24:23.319247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-04 15:24:23.319274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-04 15:24:23.320235: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-04 15:24:23.325739: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-04 15:24:23.889041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available : 0\n",
      "Mon Aug  4 15:24:24 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:AC:00.0 Off |                  Off |\n",
      "| 30%   47C    P8             33W /  300W |      34MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:CA:00.0 Off |                  Off |\n",
      "| 30%   49C    P8             31W /  300W |   18485MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2763      G   /usr/lib/xorg/Xorg                             10MiB |\n",
      "|    0   N/A  N/A      2905      G   /usr/bin/gnome-shell                            8MiB |\n",
      "|    1   N/A  N/A      2763      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   3040970      C   ...iniforge3/envs/aihub/bin/python3.10      18464MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 15:24:24.450913: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "if gpus:\n",
    "    gpu_num = 0 # Number of the GPU to be used\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n",
    "        print('Only GPU number', gpu_num, 'used.')\n",
    "        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "!nvidia-smi\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "from sionna.phy.mimo import StreamManagement\n",
    "\n",
    "#from sionna.phy.ofdm import CSIGridMapper\n",
    "from sionna.phy.ofdm import ResourceGrid, ResourceGridMapper, ResourceGridDemapper, LSChannelEstimator, LMMSEEqualizer\n",
    "from sionna.phy.ofdm import OFDMModulator, OFDMDemodulator, RemoveNulledSubcarriers, ZFEqualizer\n",
    "\n",
    "from sionna.phy.channel.tr38901 import Antenna, AntennaArray, CDL, UMi, UMa, RMa\n",
    "from sionna.phy.channel import gen_single_sector_topology as gen_topology\n",
    "from sionna.phy.channel import subcarrier_frequencies, cir_to_ofdm_channel, cir_to_time_channel\n",
    "from sionna.phy.channel import ApplyOFDMChannel, ApplyTimeChannel, OFDMChannel\n",
    "\n",
    "from sionna.phy.fec.ldpc.encoding import LDPC5GEncoder\n",
    "from sionna.phy.fec.ldpc.decoding import LDPC5GDecoder\n",
    "\n",
    "from sionna.phy.mapping import Mapper, Demapper, BinarySource, QAMSource\n",
    "\n",
    "from sionna.phy.utils import ebnodb2no, sim_ber\n",
    "from sionna.phy.utils.metrics import compute_ber\n",
    "# from sionna.ofdm import CSIGridMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f502c95c-8050-4447-8b53-08fe99f0ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx_Power: 0.1772876696043162\n",
      "Noise: -120.22878745280337\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = \"umi\"\n",
    "carrier_frequency = 5.3e9\n",
    "direction = \"uplink\"\n",
    "num_ut = 1\n",
    "batch_size = 5000\n",
    "\n",
    "num_RB   = 8\n",
    "num_symbols = 14\n",
    "num_SC = 96\n",
    "fft_size = 12*num_RB\n",
    "subcarrier_spacing = 30e3\n",
    "\n",
    "## Tx Power\n",
    "TxPower_dBm     = 20 # in dBm\n",
    "TxPower         = 10**((TxPower_dBm-30)/10)\n",
    "TxPower_SC      = TxPower/(num_SC)\n",
    "TxPower_SC_dBm  = 10*np.log10(TxPower_SC*1000)\n",
    "\n",
    "# Noise Power\n",
    "noise_psd       = -174                                  # in dBm/Hz\n",
    "noise_figure    = 9                                     # in dB\n",
    "noise_SC_Watt   = 10**((noise_psd + noise_figure - 30)/10)*subcarrier_spacing\n",
    "noise_SC_dBm    = 10*np.log10(noise_SC_Watt*1000)\n",
    "\n",
    "print(\"Tx_Power:\",TxPower_SC_dBm)\n",
    "print(\"Noise:\",noise_SC_dBm)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "# Define the UT antenna array\n",
    "ut_array = Antenna(polarization=\"single\",\n",
    "                   polarization_type=\"V\",\n",
    "                   antenna_pattern=\"omni\",\n",
    "                   carrier_frequency=carrier_frequency)\n",
    "\n",
    "# Define the BS antenna array\n",
    "bs_array = AntennaArray(num_rows=1,\n",
    "                             num_cols=1, # We want to transmitter to be equiped with the 16 rx antennas\n",
    "                             vertical_spacing=0.5,\n",
    "                             horizontal_spacing=0.5,\n",
    "                             polarization_type=\"V\",\n",
    "                             antenna_pattern=\"omni\",\n",
    "                             polarization=\"single\",\n",
    "                             carrier_frequency=7.2e9)\n",
    "\n",
    "# Create channel model\n",
    "channel_model = UMi(carrier_frequency=carrier_frequency,\n",
    "                    o2i_model=\"low\",\n",
    "                    ut_array=ut_array,\n",
    "                    bs_array=bs_array,\n",
    "                    direction=direction,\n",
    "                    enable_pathloss=True,\n",
    "                    enable_shadow_fading=True)\n",
    "\n",
    "# Generate the topology\n",
    "ut_dist = 10\n",
    "topology = gen_topology(batch_size, num_ut, scenario,min_bs_ut_dist=ut_dist, min_ut_velocity=0.33, max_ut_velocity=8.33,indoor_probability=0)\n",
    "\n",
    "# Set the topology\n",
    "channel_model.set_topology(*topology)\n",
    "ut_loc = topology[0]\n",
    "bs_loc = topology[1]\n",
    "ut_vel = topology[4]\n",
    "\n",
    "# The number of transmitted streams is equal to the number of UT antennas\n",
    "num_streams_per_tx = 1\n",
    "\n",
    "# Create an RX-TX association matrix\n",
    "# rx_tx_association[i,j]=1 means that receiver i gets at least one stream\n",
    "# from transmitter j. Depending on the transmission direction (uplink or downlink),\n",
    "# the role of UT and BS can change. However, as we have only a single\n",
    "# transmitter and receiver, this does not matter:\n",
    "rx_tx_association = np.zeros([1, num_ut])\n",
    "rx_tx_association[:, 0] = 1\n",
    "#rx_tx_association[:, 1] = 1\n",
    "\n",
    "# Instantiate a StreamManagement object\n",
    "# This determines which data streams are determined for which receiver.\n",
    "# In this simple setup, this is fairly simple. However, it can get complicated\n",
    "# for simulations with many transmitters and receivers.\n",
    "sm = StreamManagement(rx_tx_association, num_streams_per_tx)\n",
    "\n",
    "rg = ResourceGrid(num_ofdm_symbols=14,\n",
    "                  fft_size=fft_size,\n",
    "                  subcarrier_spacing=subcarrier_spacing,\n",
    "                  num_tx=num_ut,\n",
    "                  num_streams_per_tx=num_streams_per_tx,\n",
    "                  cyclic_prefix_length=0,\n",
    "                  pilot_pattern = None,\n",
    "                  pilot_ofdm_symbol_indices = None)\n",
    "rg.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8812cf80-3d12-4118-97b2-adad62b608ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bits_per_symbol = 2 # QPSK modulation\n",
    "coderate = 0.3 # The code rate\n",
    "n = int(rg.num_data_symbols*num_bits_per_symbol)  # Number of coded bits\n",
    "k = int(n*coderate) # Number of information bits\n",
    "\n",
    "# The binary source will create batches of information bits\n",
    "binary_source = BinarySource()\n",
    "qam_source = QAMSource(num_bits_per_symbol)\n",
    "\n",
    "# The encoder maps information bits to coded bits\n",
    "encoder = LDPC5GEncoder(k, n)\n",
    "\n",
    "# The mapper maps blocks of information bits to constellation symbols\n",
    "mapper = Mapper(\"qam\", num_bits_per_symbol)\n",
    "\n",
    "# The resource grid mapper maps symbols onto an OFDM resource grid\n",
    "rg_mapper = ResourceGridMapper(rg)\n",
    "\n",
    "# CSI-RS mapper\n",
    "#csi_mapper = CSIGridMapper(rg)\n",
    "\n",
    "#rg_demap = ResourceGridDemapper(rg, sm)\n",
    "\n",
    "# This function removes nulled subcarriers from any tensor having the shape of a resource grid\n",
    "remove_nulled_scs = RemoveNulledSubcarriers(rg)\n",
    "\n",
    "# CSI-RS estimator will provide the raw channel estimates\n",
    "#csi_est = CSIrsChannelEstimator(rg, sm)\n",
    "#csi_est = CSIrsChannelEstimator(rg, sm, TxPower_SC, interpolation_type = 'lin')\n",
    "\n",
    "# The LMMSE equalizer will provide soft symbols together with noise variance estimates\n",
    "zf_equ = ZFEqualizer(rg, sm)\n",
    "\n",
    "# The demapper produces LLR for all coded bits\n",
    "demapper = Demapper(\"app\", \"qam\", num_bits_per_symbol)\n",
    "\n",
    "# The decoder provides hard-decisions on the information bits\n",
    "decoder = LDPC5GDecoder(encoder, hard_out=True)\n",
    "\n",
    "# OFDM CHannel\n",
    "ofdm_channel = OFDMChannel(channel_model, rg, add_awgn=True, normalize_channel=False, return_channel=True)\n",
    "channel_freq = ApplyOFDMChannel(add_awgn=True)\n",
    "frequencies = subcarrier_frequencies(rg.fft_size, rg.subcarrier_spacing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a7457-9bf5-4bfc-9062-c2655bcebc6a",
   "metadata": {},
   "source": [
    "### Classical Communication System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49cb33a-8212-4234-9ae8-35774ccc7f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received Signal Shape :  (5000, 1, 1, 14, 96)\n",
      "CSI Est NMSE :  tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "BER: 0.03471822916666667\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "## Transmitter \n",
    "b = binary_source([batch_size, num_ut, rg.num_streams_per_tx, n])\n",
    "#c = encoder(b)                ## All bits are information bits for now\n",
    "\n",
    "x = mapper(b)                  # Mapping bits to modulation symbols\n",
    "x_rg = rg_mapper(x)            # Mapping symbols to OFDM resources\n",
    "x_rg = np.sqrt(TxPower_SC) * x_rg      # Assigning transmit power\n",
    "\n",
    "## Passing through wireless channel\n",
    "a, tau = channel_model(num_time_samples=rg.num_ofdm_symbols, sampling_frequency=1/rg.ofdm_symbol_duration)\n",
    "h_freq = cir_to_ofdm_channel(frequencies, a, tau, normalize=False)\n",
    "\n",
    "#Receiver (Signal + Channel + Thermal Noise : AWGN)\n",
    "no = noise_SC_Watt\n",
    "y = channel_freq(x_rg, h_freq, no)            ## This is your input to Neural Rx: Received Signal, Optional Inputs : H_Freq, No\n",
    "print(\"Received Signal Shape : \", y.shape)\n",
    "\n",
    "# Channel Estimation\n",
    "returnCompleteRG = True;    num_ant_ports = 1\n",
    "#h_freq_d_DL_hat, err_var = csi_est([y, no, num_ant_ports, returnCompleteRG])\n",
    "h_freq_d_DL_hat = h_freq\n",
    "\n",
    "#x_hat = tf.squeeze(tf.matmul(g, y), axis=-1)\n",
    "x_hat, no_eff = zf_equ(y, h_freq_d_DL_hat, tf.zeros(h_freq_d_DL_hat.shape), no) \n",
    "x_hat = x_hat / np.sqrt(TxPower_SC)\n",
    "\n",
    "llr = demapper(x_hat, no_eff)       ## This is your ground truth label for Neural Rx\n",
    "\n",
    "#b_hat = decoder(llr)\n",
    "b_hat = tf.cast(tf.less(0.0, llr), llr.dtype)\n",
    "\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = tf.reduce_mean(tf.abs(h_freq_d_DL_hat - h_freq)**2)\n",
    "print(\"CSI Est NMSE : \", mse / tf.reduce_mean(tf.abs(h_freq)**2))\n",
    "\n",
    "print(\"BER: {}\".format(compute_ber(b, b_hat).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904c6e9-fcd6-440a-810c-796c6887085a",
   "metadata": {},
   "source": [
    "### Neural Rx: NVIDIA Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f275cd-ccd7-4a1c-b365-69f79307d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rx Signal Power :  tf.Tensor(3.2747136e-12, shape=(), dtype=float32)\n",
      "Channel Power :  tf.Tensor(3.1428478e-09, shape=(), dtype=float32)\n",
      "(None, 14, 96, 4)\n",
      "Epoch 1/50\n",
      "(32, 14, 96, 4)\n",
      "(32, 14, 96, 4)\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9714(None, 14, 96, 4)\n",
      "125/125 [==============================] - 19s 139ms/step - loss: 0.9714 - val_loss: 0.6863\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.6083 - val_loss: 0.5220\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.4664 - val_loss: 0.4230\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.3214 - val_loss: 0.2305\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.2036 - val_loss: 0.1900\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.1713 - val_loss: 0.1688\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.1363 - val_loss: 0.1097\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0960 - val_loss: 0.0881\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0774 - val_loss: 0.0800\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0745 - val_loss: 0.0826\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0671 - val_loss: 0.0710\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0629 - val_loss: 0.0677\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0631 - val_loss: 0.0673\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0599 - val_loss: 0.0697\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0575 - val_loss: 0.0665\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0564 - val_loss: 0.0655\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0542 - val_loss: 0.0681\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0518 - val_loss: 0.0667\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0509 - val_loss: 0.0651\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0506 - val_loss: 0.0630\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0487 - val_loss: 0.0622\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0473 - val_loss: 0.0627\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0456 - val_loss: 0.0634\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0440 - val_loss: 0.0676\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0442 - val_loss: 0.0591\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0425 - val_loss: 0.0652\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 17s 140ms/step - loss: 0.0431 - val_loss: 0.0602\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0390 - val_loss: 0.0613\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0394 - val_loss: 0.0619\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0395 - val_loss: 0.0646\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0370 - val_loss: 0.0602\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0364 - val_loss: 0.0643\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0353 - val_loss: 0.0619\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0332 - val_loss: 0.0657\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0344 - val_loss: 0.0712\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0314 - val_loss: 0.0650\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0292 - val_loss: 0.0732\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0323 - val_loss: 0.0630\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0280 - val_loss: 0.0664\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0290 - val_loss: 0.0655\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0276 - val_loss: 0.0735\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0261 - val_loss: 0.0740\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0261 - val_loss: 0.0671\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0267 - val_loss: 0.0704\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0263 - val_loss: 0.0900\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0259 - val_loss: 0.0819\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0207 - val_loss: 0.0743\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0218 - val_loss: 0.0818\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0208 - val_loss: 0.0775\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.0196 - val_loss: 0.0732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b9f7c290b20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "# For the implementation of the neural receiver\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "num_conv_channels = 128\n",
    "n = int(rg.num_data_symbols*num_bits_per_symbol)\n",
    "class ResidualBlock(Layer):\n",
    "    def build(self, input_shape):\n",
    "        self._layer_norm_1 = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self._conv_1 = Conv2D(filters=num_conv_channels, kernel_size=[3,3], padding='same', activation=None)\n",
    "        self._layer_norm_2 = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self._conv_2 = Conv2D(filters=num_conv_channels, kernel_size=[3,3], padding='same', activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = self._layer_norm_1(inputs)\n",
    "        z = relu(z)\n",
    "        z = self._conv_1(z)\n",
    "        z = self._layer_norm_2(z)\n",
    "        z = relu(z)\n",
    "        z = self._conv_2(z)\n",
    "        z = z + inputs  # skip connection\n",
    "        return z\n",
    "\n",
    "\n",
    "# NeuralReceiver with y and h inputs\n",
    "class NeuralReceiver(Layer):\n",
    "    def build(self, input_shape):\n",
    "        self._input_conv = Conv2D(filters=num_conv_channels, kernel_size=3, padding='same', activation=None)\n",
    "        self._res_block_1 = ResidualBlock()\n",
    "        self._res_block_2 = ResidualBlock()\n",
    "        self._res_block_3 = ResidualBlock()\n",
    "        self._res_block_4 = ResidualBlock()\n",
    "        self._output_conv = Conv2D(filters=num_bits_per_symbol, kernel_size=3, padding='same', activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y, h = inputs  # y: (batch,1,num_symbols,num_SC), h: (batch,1,1,1,1,num_symbols,num_SC)\n",
    "        batch_size = tf.shape(y)[0]\n",
    "\n",
    "        # transpose to (batch, num_symbols, num_SC, 1)\n",
    "        y = tf.transpose(y, [0, 2, 3, 1])  # (batch,num_symbols,num_SC,1)\n",
    "        h = tf.transpose(h, [0, 2, 3, 1])  # same shape as y\n",
    "\n",
    "         # === Only keep symbol index 1 ===\n",
    "        symbol_mask = tf.one_hot(1, depth=tf.shape(h)[1], dtype=h.dtype)  # shape (num_symbols,)\n",
    "        symbol_mask = tf.reshape(symbol_mask, [1, -1, 1, 1])  # shape (1, num_symbols, 1, 1)\n",
    "        h = h * symbol_mask  # broadcasting to mask out all symbols except index 1\n",
    "\n",
    "        # === Add noise to h to achieve NMSE = 0.001 === (Synthetci Channel Estimation Effect)\n",
    "        # === Add noise to h to achieve NMSE = 0.001 ===\n",
    "        h_power = tf.reduce_mean(tf.abs(h)**2)\n",
    "        noise_power = 0.001 * h_power\n",
    "\n",
    "        # Generate complex Gaussian noise\n",
    "        noise_real = tf.random.normal(tf.shape(h), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "        noise_imag = tf.random.normal(tf.shape(h), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "        noise = tf.complex(noise_real, noise_imag)  # tf.complex64\n",
    "\n",
    "        # Normalize noise to unit power and then scale\n",
    "        noise_norm_factor = tf.sqrt(tf.reduce_mean(tf.abs(noise)**2))  # real-valued\n",
    "        noise = noise / tf.cast(noise_norm_factor, tf.complex64)       # safe division\n",
    "        noise = noise * tf.cast(tf.sqrt(noise_power), tf.complex64)\n",
    "\n",
    "        h = h + noise  # Noisy channel now has NMSE  0.1\n",
    "        # Normalize y and h power\n",
    "        y_power = tf.reduce_mean(tf.abs(y), axis=[1,2,3], keepdims=True) + 1e-6\n",
    "        h_power = tf.reduce_mean(tf.abs(h), axis=[1,2,3], keepdims=True) + 1e-6\n",
    "        y_norm = y / tf.cast(y_power, y.dtype)\n",
    "        h_norm = h / tf.cast(h_power, h.dtype)\n",
    "\n",
    "        # Separate real and imag parts and concat last dim\n",
    "        y_ri = tf.concat([tf.math.real(y_norm), tf.math.imag(y_norm)], axis=-1)  # (batch,num_symbols,num_SC,2)\n",
    "        h_ri = tf.concat([tf.math.real(h_norm), tf.math.imag(h_norm)], axis=-1)  # (batch,num_symbols,num_SC,2)\n",
    "        #h_ri = tf.zeros((batch_size, num_symbols, num_SC, 2), dtype=tf.float32)\n",
    "\n",
    "        # Concatenate y and h along last dim => shape (batch,num_symbols,num_SC,4)\n",
    "        z = tf.concat([y_ri, h_ri], axis=-1)\n",
    "        print(z.shape)\n",
    "\n",
    "        # Pass through conv and residual blocks\n",
    "        z = self._input_conv(z)\n",
    "        z = self._res_block_1(z)\n",
    "        z = self._res_block_2(z)\n",
    "        z = self._res_block_3(z)\n",
    "        z = self._res_block_4(z)\n",
    "        z = self._output_conv(z)\n",
    "\n",
    "        # Flatten output to (batch, 1, 1, n)\n",
    "        z = tf.reshape(z, [batch_size, 1, 1, num_symbols * num_SC * num_bits_per_symbol])\n",
    "        return z\n",
    "\n",
    "def binary_sigmoid_cross_entropy(bit_labels: tf.Tensor, pred_llr: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss with sigmoid applied to predicted LLRs,\n",
    "    ignoring invalid bits labeled as -1.\n",
    "\n",
    "    Args:\n",
    "        bit_labels: Tensor of shape [batch, symbols, subcarriers, bits], with labels in {0,1} or -1 for invalid bits.\n",
    "        pred_llr: Tensor of same shape, raw logits (LLRs).\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor: average loss over valid bits only.\n",
    "    \"\"\"\n",
    "    bit_labels = tf.cast(bit_labels, pred_llr.dtype)\n",
    "\n",
    "    # Find valid bits mask (exclude bits with label -1)\n",
    "    valid_mask = tf.not_equal(bit_labels, -1)\n",
    "\n",
    "    # Apply sigmoid to logits to get bit probabilities\n",
    "    bit_prob = tf.sigmoid(pred_llr)\n",
    "\n",
    "    # Mask out invalid bits in predictions and labels\n",
    "    bit_prob_masked = tf.boolean_mask(bit_prob, valid_mask)\n",
    "    bit_labels_masked = tf.boolean_mask(bit_labels, valid_mask)\n",
    "\n",
    "    # Compute binary cross-entropy loss\n",
    "    bce = tf.keras.losses.binary_crossentropy(bit_labels_masked, bit_prob_masked)\n",
    "\n",
    "    # Average loss over valid bits\n",
    "    loss = tf.reduce_mean(bce)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Build the model inputs\n",
    "input_y = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "input_h = tf.keras.Input(shape=(1, num_symbols, num_SC), dtype=tf.complex64)\n",
    "\n",
    "\n",
    "# === Prepare your data ===\n",
    "# Assuming you have tensors: y (batch,1,1,num_symbols,num_SC), h_freq (batch,1,1,1,1,num_symbols,num_SC), llr (batch,num_symbols,num_SC,num_bits_per_symbol)\n",
    "\n",
    "# Example for llr_probs preprocessing\n",
    "llr_probs = tf.sigmoid(llr)\n",
    "llr_probs = tf.reshape(llr_probs, [llr_probs.shape[0], 1, 1, -1])  # flatten bits, symbols and SC into last dim\n",
    "\n",
    "# Squeeze and reshape y and h properly\n",
    "y_data = tf.squeeze(y, axis=(2))              # from (N,1,1,14,96) to (N,1,14,96)\n",
    "print(\"Rx Signal Power : \", tf.reduce_mean(tf.abs(y)**2))\n",
    "\n",
    "h_data = tf.squeeze(h_freq, axis=[1,2,3]) # from (N,1,1,1,1,14,96) to (N,14,96)\n",
    "print(\"Channel Power : \", tf.reduce_mean(tf.abs(h_freq)**2))\n",
    "#h_data = tf.expand_dims(h_data, axis=1)    # (N,1,14,96)\n",
    "\n",
    "# Instantiate model\n",
    "receiver = NeuralReceiver()\n",
    "output_llrs = receiver([input_y, input_h])\n",
    "receiver_model = Model(inputs=[input_y, input_h], outputs=output_llrs)\n",
    "# Compile\n",
    "receiver_model.compile(optimizer=Adam(learning_rate=0.001), loss=binary_sigmoid_cross_entropy)\n",
    "\n",
    "# Now split train/val\n",
    "dataset_size = y_data.shape[0]\n",
    "indices = tf.random.shuffle(tf.range(dataset_size))\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "y_train = tf.gather(y_data, train_indices)  # shape (train_size, 1, 14, 96)\n",
    "y_val = tf.gather(y_data, val_indices)\n",
    "\n",
    "h_train = tf.gather(h_data, train_indices)  # shape (train_size, 1, 14, 96)\n",
    "h_val = tf.gather(h_data, val_indices)\n",
    "\n",
    "llr_train = tf.gather(llr_probs, train_indices)\n",
    "llr_val = tf.gather(llr_probs, val_indices)\n",
    "\n",
    "# Train\n",
    "receiver_model.fit(\n",
    "    [y_train, h_train],\n",
    "    llr_train,\n",
    "    validation_data=([y_val, h_val], llr_val),\n",
    "    batch_size=32,\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0edfbc66-d2b5-4e06-bb8a-3de3d17fcbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 14, 96, 4)\n",
      "Neural Receiver BER (validation): 0.045847\n",
      "ZF Baseline BER (validation): 0.035301\n",
      "Relative BER change vs ZF: +29.88%\n"
     ]
    }
   ],
   "source": [
    "# ==== Evaluate after training (validation split) ====\n",
    "\n",
    "# 1) Predict LLRs with the neural receiver\n",
    "llr_pred = receiver_model.predict([y_val, h_val], batch_size=32, verbose=0)  # (valN, 1, 1, total_bits)\n",
    "\n",
    "# 2) Hard decisions -> predicted bits {0,1}; flatten to (valN, total_bits)\n",
    "bits_pred_flat = tf.reshape(tf.cast(llr_pred > 0.0, tf.float32), [tf.shape(llr_pred)[0], -1])\n",
    "\n",
    "# 3) Ground-truth bits for the SAME validation subset; flatten to (valN, total_bits)\n",
    "#    b shape from Tx step: (N, num_ut, rg.num_streams_per_tx, n_total_bits)\n",
    "b_all_flat = tf.squeeze(b, axis=[1, 2])                                # -> (N, n_total_bits)\n",
    "b_val_flat = tf.cast(tf.gather(b_all_flat, val_indices), tf.float32)   # -> (valN, n_total_bits)\n",
    "\n",
    "# 4) Neural receiver BER\n",
    "ber_neural = compute_ber(b_val_flat, bits_pred_flat).numpy()\n",
    "print(f\"Neural Receiver BER (validation): {ber_neural:.6f}\")\n",
    "\n",
    "# ----- Optional: compare with your classical ZF baseline on the SAME val split -----\n",
    "# If you still have 'llr' from the ZF demapper used earlier:\n",
    "try:\n",
    "    # llr shape typically: (N, num_symbols, num_SC, num_bits_per_symbol)\n",
    "    bits_zf = tf.cast(llr > 0.0, tf.float32)                           # -> (N, ..., ...)\n",
    "    bits_zf_flat = tf.reshape(bits_zf, [tf.shape(bits_zf)[0], -1])     # -> (N, n_total_bits)\n",
    "    bits_zf_val_flat = tf.gather(bits_zf_flat, val_indices)            # -> (valN, n_total_bits)\n",
    "\n",
    "    ber_zf = compute_ber(b_val_flat, bits_zf_val_flat).numpy()\n",
    "    print(f\"ZF Baseline BER (validation): {ber_zf:.6f}\")\n",
    "    if ber_zf > 0:\n",
    "        print(f\"Relative BER change vs ZF: {(ber_neural - ber_zf) / ber_zf:+.2%}\")\n",
    "except Exception as e:\n",
    "    print(\"ZF baseline comparison skipped (llr not available or shape mismatch).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc42c94-0be2-48e3-82f3-62a036815404",
   "metadata": {},
   "source": [
    "### NI's Version of DeepRX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3ba4fe-ce3e-4b24-acb0-dbdaeb479713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlockProperties:\n",
    "    \"\"\"Structure that holds all configurational parameters of the ResNet blocks.\"\"\"\n",
    "\n",
    "    num_blocks: int = 0\n",
    "    kernel_size: list = []\n",
    "    dilation_rate: list = []\n",
    "    num_filter: list = []\n",
    "\n",
    "class DeepRx:\n",
    "    \"\"\"DeepRx neural network model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_ofdm_sym: int,\n",
    "        num_subcar: int,\n",
    "        num_ant: int,\n",
    "        res_net_config: ResNetBlockProperties,\n",
    "        num_output_llr: int,\n",
    "        use_submodels: bool,\n",
    "    ):\n",
    "        \"\"\"Initializes the network topology\"\"\"\n",
    "\n",
    "        # number of symbols in time: S\n",
    "        self.num_ofdm_sym = num_ofdm_sym\n",
    "        # number of subcarriers: F\n",
    "        self.num_subcar = num_subcar\n",
    "        # number of antennas: N_r\n",
    "        self.num_ant = num_ant\n",
    "        # number of output LLRs: B\n",
    "        self.num_output_llr = num_output_llr\n",
    "\n",
    "        # RX data input: complex value already split into two channels\n",
    "        y = tf.keras.layers.Input(shape=(num_ofdm_sym, num_subcar, 2 * num_ant), name=\"RX-Data-In\")\n",
    "        # TX pilots: complex value already split into two channels\n",
    "        x_p = tf.keras.layers.Input(shape=(num_ofdm_sym, num_subcar, 2), name=\"TX-Pilot-In\")\n",
    "        # raw channel estimate: complex value already split into two channels\n",
    "        h_r = tf.keras.layers.Input(shape=(num_ofdm_sym, num_subcar, 2 * num_ant), name=\"Raw-Channel-Est-In\")\n",
    "        # concatenate input layers\n",
    "        concat = tf.keras.layers.concatenate([y, x_p, h_r], name=\"Concat\")\n",
    "        # convolutional input layer\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), dilation_rate=(1, 1), padding=\"same\", activation=None, name=\"Conv-In\")(\n",
    "            concat\n",
    "        )\n",
    "        # construct ResNet blocks\n",
    "        for block_idx in range(res_net_config.num_blocks):\n",
    "            # generate ResNet block as compact sub-models\n",
    "            if use_submodels:\n",
    "                _, res_net_block = self.create_res_net_block(\n",
    "                    input=x,\n",
    "                    filter_size=tuple(res_net_config.kernel_size[block_idx]),\n",
    "                    dilation=tuple(res_net_config.dilation_rate[block_idx]),\n",
    "                    num_filter=res_net_config.num_filter[block_idx],\n",
    "                    res_net_block_idx=block_idx,\n",
    "                )\n",
    "                x = res_net_block(x)\n",
    "            # generate ResNet block with all sub-layers visible\n",
    "            else:\n",
    "                x, _ = self.create_res_net_block(\n",
    "                    input=x,\n",
    "                    filter_size=tuple(res_net_config.kernel_size[block_idx]),\n",
    "                    dilation=tuple(res_net_config.dilation_rate[block_idx]),\n",
    "                    num_filter=res_net_config.num_filter[block_idx],\n",
    "                    res_net_block_idx=block_idx,\n",
    "                )\n",
    "        # convolutional output layer: LLR\n",
    "        output = tf.keras.layers.Conv2D(\n",
    "            self.num_output_llr, (3, 3), dilation_rate=(1, 1), padding=\"same\", activation=None, name=\"Conv-Out\"\n",
    "        )(x)\n",
    "        # instantiate complete model\n",
    "        self.model = tf.keras.Model(inputs=[y, x_p, h_r], outputs=output, name=\"DeepRx\")\n",
    "\n",
    "    def create_res_net_block(\n",
    "        self,\n",
    "        input: tf.Tensor,\n",
    "        filter_size: tuple,\n",
    "        dilation: tuple,\n",
    "        num_filter: int,\n",
    "        res_net_block_idx: int,\n",
    "    ):\n",
    "        \"\"\"Creates the ResNet sub-blocks\"\"\"\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"BN-{res_net_block_idx}-0\")(input)\n",
    "        x = tf.keras.layers.ReLU(name=f\"ReLU-{res_net_block_idx}-0\")(x)\n",
    "        x = tf.keras.layers.SeparableConv2D(\n",
    "            filters=num_filter,\n",
    "            kernel_size=filter_size,\n",
    "            dilation_rate=dilation,\n",
    "            depth_multiplier=1,\n",
    "            padding=\"same\",\n",
    "            name=f\"Separable-Conv-{res_net_block_idx}-0\",\n",
    "        )(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"BN-{res_net_block_idx}-1\")(x)\n",
    "        x = tf.keras.layers.ReLU(name=f\"ReLU-{res_net_block_idx}-1\")(x)\n",
    "        x = tf.keras.layers.SeparableConv2D(\n",
    "            filters=num_filter,\n",
    "            kernel_size=filter_size,\n",
    "            dilation_rate=dilation,\n",
    "            depth_multiplier=1,\n",
    "            padding=\"same\",\n",
    "            name=f\"Separable-Conv-{res_net_block_idx}-1\",\n",
    "        )(x)\n",
    "\n",
    "        # When ResNet block's output depth is increased or decreased, also the residual path has to be up- or downsampled.\n",
    "        # This can be achieved via 1x1 convolutions.\n",
    "        if input.shape[3] != num_filter:\n",
    "            x_shortcut = tf.keras.layers.Conv2D(num_filter, (1, 1), name=f\"Conv-Depth-Adjust-{res_net_block_idx}\")(\n",
    "                input\n",
    "            )\n",
    "            # TODO: do we need additional BN and ReLU activation here?\n",
    "            x_shortcut = tf.keras.layers.BatchNormalization(name=f\"BN-{res_net_block_idx}-2\")(x_shortcut)\n",
    "            # x_shortcut = tf.keras.layers.ReLU(name=f\"ReLU-{res_net_block_idx}-2\")(x_shortcut)\n",
    "        else:\n",
    "            x_shortcut = input\n",
    "\n",
    "        output = tf.keras.layers.Add(name=f\"Add-{res_net_block_idx}\")([x_shortcut, x])\n",
    "\n",
    "        res_net_model = tf.keras.Model(inputs=input, outputs=output, name=f\"ResNet-Block-{res_net_block_idx}\")\n",
    "\n",
    "        return output, res_net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b30d4f-9be5-400e-abdd-b834e2dd59ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepRx\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " RX-Data-In (InputLayer)     [(None, 14, 96, 2)]          0         []                            \n",
      "                                                                                                  \n",
      " TX-Pilot-In (InputLayer)    [(None, 14, 96, 2)]          0         []                            \n",
      "                                                                                                  \n",
      " Raw-Channel-Est-In (InputL  [(None, 14, 96, 2)]          0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " Concat (Concatenate)        (None, 14, 96, 6)            0         ['RX-Data-In[0][0]',          \n",
      "                                                                     'TX-Pilot-In[0][0]',         \n",
      "                                                                     'Raw-Channel-Est-In[0][0]']  \n",
      "                                                                                                  \n",
      " Conv-In (Conv2D)            (None, 14, 96, 64)           3520      ['Concat[0][0]']              \n",
      "                                                                                                  \n",
      " BN-0-0 (BatchNormalization  (None, 14, 96, 64)           256       ['Conv-In[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-0-0 (ReLU)             (None, 14, 96, 64)           0         ['BN-0-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-0-0 (Separa  (None, 14, 96, 64)           4736      ['ReLU-0-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-0-1 (BatchNormalization  (None, 14, 96, 64)           256       ['Separable-Conv-0-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-0-1 (ReLU)             (None, 14, 96, 64)           0         ['BN-0-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-0-1 (Separa  (None, 14, 96, 64)           4736      ['ReLU-0-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-0 (Add)                 (None, 14, 96, 64)           0         ['Conv-In[0][0]',             \n",
      "                                                                     'Separable-Conv-0-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-1-0 (BatchNormalization  (None, 14, 96, 64)           256       ['Add-0[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-1-0 (ReLU)             (None, 14, 96, 64)           0         ['BN-1-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-1-0 (Separa  (None, 14, 96, 64)           4736      ['ReLU-1-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-1-1 (BatchNormalization  (None, 14, 96, 64)           256       ['Separable-Conv-1-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-1-1 (ReLU)             (None, 14, 96, 64)           0         ['BN-1-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-1-1 (Separa  (None, 14, 96, 64)           4736      ['ReLU-1-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-1 (Add)                 (None, 14, 96, 64)           0         ['Add-0[0][0]',               \n",
      "                                                                     'Separable-Conv-1-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-2-0 (BatchNormalization  (None, 14, 96, 64)           256       ['Add-1[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-2-0 (ReLU)             (None, 14, 96, 64)           0         ['BN-2-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-2-0 (Separa  (None, 14, 96, 96)           6816      ['ReLU-2-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-2-1 (BatchNormalization  (None, 14, 96, 96)           384       ['Separable-Conv-2-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Conv-Depth-Adjust-2 (Conv2  (None, 14, 96, 96)           6240      ['Add-1[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " ReLU-2-1 (ReLU)             (None, 14, 96, 96)           0         ['BN-2-1[0][0]']              \n",
      "                                                                                                  \n",
      " BN-2-2 (BatchNormalization  (None, 14, 96, 96)           384       ['Conv-Depth-Adjust-2[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Separable-Conv-2-1 (Separa  (None, 14, 96, 96)           10176     ['ReLU-2-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-2 (Add)                 (None, 14, 96, 96)           0         ['BN-2-2[0][0]',              \n",
      "                                                                     'Separable-Conv-2-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-3-0 (BatchNormalization  (None, 14, 96, 96)           384       ['Add-2[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-3-0 (ReLU)             (None, 14, 96, 96)           0         ['BN-3-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-3-0 (Separa  (None, 14, 96, 96)           10176     ['ReLU-3-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-3-1 (BatchNormalization  (None, 14, 96, 96)           384       ['Separable-Conv-3-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-3-1 (ReLU)             (None, 14, 96, 96)           0         ['BN-3-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-3-1 (Separa  (None, 14, 96, 96)           10176     ['ReLU-3-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-3 (Add)                 (None, 14, 96, 96)           0         ['Add-2[0][0]',               \n",
      "                                                                     'Separable-Conv-3-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-4-0 (BatchNormalization  (None, 14, 96, 96)           384       ['Add-3[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-4-0 (ReLU)             (None, 14, 96, 96)           0         ['BN-4-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-4-0 (Separa  (None, 14, 96, 128)          13280     ['ReLU-4-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-4-1 (BatchNormalization  (None, 14, 96, 128)          512       ['Separable-Conv-4-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Conv-Depth-Adjust-4 (Conv2  (None, 14, 96, 128)          12416     ['Add-3[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " ReLU-4-1 (ReLU)             (None, 14, 96, 128)          0         ['BN-4-1[0][0]']              \n",
      "                                                                                                  \n",
      " BN-4-2 (BatchNormalization  (None, 14, 96, 128)          512       ['Conv-Depth-Adjust-4[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Separable-Conv-4-1 (Separa  (None, 14, 96, 128)          17664     ['ReLU-4-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-4 (Add)                 (None, 14, 96, 128)          0         ['BN-4-2[0][0]',              \n",
      "                                                                     'Separable-Conv-4-1[0][0]']  \n",
      "                                                                                                  \n",
      " BN-5-0 (BatchNormalization  (None, 14, 96, 128)          512       ['Add-4[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-5-0 (ReLU)             (None, 14, 96, 128)          0         ['BN-5-0[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-5-0 (Separa  (None, 14, 96, 128)          17664     ['ReLU-5-0[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " BN-5-1 (BatchNormalization  (None, 14, 96, 128)          512       ['Separable-Conv-5-0[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " ReLU-5-1 (ReLU)             (None, 14, 96, 128)          0         ['BN-5-1[0][0]']              \n",
      "                                                                                                  \n",
      " Separable-Conv-5-1 (Separa  (None, 14, 96, 128)          17664     ['ReLU-5-1[0][0]']            \n",
      " bleConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " Add-5 (Add)                 (None, 14, 96, 128)          0         ['Add-4[0][0]',               \n",
      "                                                                     'Separable-Conv-5-1[0][0]']  \n",
      "                                                                                                  \n",
      " Conv-Out (Conv2D)           (None, 14, 96, 2)            2306      ['Add-5[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 152290 (594.88 KB)\n",
      "Trainable params: 149666 (584.63 KB)\n",
      "Non-trainable params: 2624 (10.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deeprx = DeepRx(num_ofdm_sym=num_symbols,\n",
    "                num_subcar=num_SC,\n",
    "                num_ant=1,\n",
    "                res_net_config=cfg,\n",
    "                num_output_llr=num_bits_per_symbol,\n",
    "                use_submodels=False).model\n",
    "\n",
    "deeprx.summary()  # Verify output shape is (None, S, F, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d6085ca-e24c-4632-a45f-edc1f4c85f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "y_train: (4000, 14, 96, 2) \n",
      "h_train: (4000, 14, 96, 2) \n",
      "x_p_train: (4000, 14, 96, 2) \n",
      "labels: (4000, 14, 96, 2)\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5348 - binary_accuracy: 0.6931\n",
      "Epoch 1: val_loss improved from inf to 0.68350, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 28s 200ms/step - loss: 0.5348 - binary_accuracy: 0.6931 - val_loss: 0.6835 - val_binary_accuracy: 0.5022 - lr: 0.0010\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dm57767/miniconda3/envs/sionna_OAI/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - ETA: 0s - loss: 0.4571 - binary_accuracy: 0.7663\n",
      "Epoch 2: val_loss improved from 0.68350 to 0.62458, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.4571 - binary_accuracy: 0.7663 - val_loss: 0.6246 - val_binary_accuracy: 0.5607 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4308 - binary_accuracy: 0.7833\n",
      "Epoch 3: val_loss improved from 0.62458 to 0.50287, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.4308 - binary_accuracy: 0.7833 - val_loss: 0.5029 - val_binary_accuracy: 0.7198 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4113 - binary_accuracy: 0.7933\n",
      "Epoch 4: val_loss improved from 0.50287 to 0.41655, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.4113 - binary_accuracy: 0.7933 - val_loss: 0.4166 - val_binary_accuracy: 0.7966 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4018 - binary_accuracy: 0.7995\n",
      "Epoch 5: val_loss improved from 0.41655 to 0.40704, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.4018 - binary_accuracy: 0.7995 - val_loss: 0.4070 - val_binary_accuracy: 0.8035 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3868 - binary_accuracy: 0.8075\n",
      "Epoch 6: val_loss improved from 0.40704 to 0.38006, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3868 - binary_accuracy: 0.8075 - val_loss: 0.3801 - val_binary_accuracy: 0.8179 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3717 - binary_accuracy: 0.8176\n",
      "Epoch 7: val_loss improved from 0.38006 to 0.36710, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.3717 - binary_accuracy: 0.8176 - val_loss: 0.3671 - val_binary_accuracy: 0.8234 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3509 - binary_accuracy: 0.8319\n",
      "Epoch 8: val_loss did not improve from 0.36710\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.3509 - binary_accuracy: 0.8319 - val_loss: 0.3732 - val_binary_accuracy: 0.8326 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3442 - binary_accuracy: 0.8349\n",
      "Epoch 9: val_loss improved from 0.36710 to 0.35674, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.3442 - binary_accuracy: 0.8349 - val_loss: 0.3567 - val_binary_accuracy: 0.8370 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3258 - binary_accuracy: 0.8447\n",
      "Epoch 10: val_loss improved from 0.35674 to 0.32449, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.3258 - binary_accuracy: 0.8447 - val_loss: 0.3245 - val_binary_accuracy: 0.8511 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3026 - binary_accuracy: 0.8569\n",
      "Epoch 11: val_loss improved from 0.32449 to 0.31810, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.3026 - binary_accuracy: 0.8569 - val_loss: 0.3181 - val_binary_accuracy: 0.8588 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2774 - binary_accuracy: 0.8703\n",
      "Epoch 12: val_loss improved from 0.31810 to 0.25216, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.2774 - binary_accuracy: 0.8703 - val_loss: 0.2522 - val_binary_accuracy: 0.8882 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2333 - binary_accuracy: 0.8930\n",
      "Epoch 13: val_loss improved from 0.25216 to 0.23697, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.2333 - binary_accuracy: 0.8930 - val_loss: 0.2370 - val_binary_accuracy: 0.8991 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2057 - binary_accuracy: 0.9070\n",
      "Epoch 14: val_loss improved from 0.23697 to 0.22170, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.2057 - binary_accuracy: 0.9070 - val_loss: 0.2217 - val_binary_accuracy: 0.9065 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1918 - binary_accuracy: 0.9137\n",
      "Epoch 15: val_loss improved from 0.22170 to 0.18309, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.1918 - binary_accuracy: 0.9137 - val_loss: 0.1831 - val_binary_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1774 - binary_accuracy: 0.9212\n",
      "Epoch 16: val_loss improved from 0.18309 to 0.18184, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.1774 - binary_accuracy: 0.9212 - val_loss: 0.1818 - val_binary_accuracy: 0.9199 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1668 - binary_accuracy: 0.9262\n",
      "Epoch 17: val_loss did not improve from 0.18184\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.1668 - binary_accuracy: 0.9262 - val_loss: 0.2442 - val_binary_accuracy: 0.9085 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1463 - binary_accuracy: 0.9352\n",
      "Epoch 18: val_loss improved from 0.18184 to 0.16006, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.1463 - binary_accuracy: 0.9352 - val_loss: 0.1601 - val_binary_accuracy: 0.9313 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1479 - binary_accuracy: 0.9344\n",
      "Epoch 19: val_loss improved from 0.16006 to 0.14716, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.1479 - binary_accuracy: 0.9344 - val_loss: 0.1472 - val_binary_accuracy: 0.9357 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1369 - binary_accuracy: 0.9401\n",
      "Epoch 20: val_loss improved from 0.14716 to 0.14368, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.1369 - binary_accuracy: 0.9401 - val_loss: 0.1437 - val_binary_accuracy: 0.9369 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1309 - binary_accuracy: 0.9423\n",
      "Epoch 21: val_loss did not improve from 0.14368\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.1309 - binary_accuracy: 0.9423 - val_loss: 0.1613 - val_binary_accuracy: 0.9297 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1261 - binary_accuracy: 0.9446\n",
      "Epoch 22: val_loss improved from 0.14368 to 0.11531, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.1261 - binary_accuracy: 0.9446 - val_loss: 0.1153 - val_binary_accuracy: 0.9505 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1163 - binary_accuracy: 0.9490\n",
      "Epoch 23: val_loss did not improve from 0.11531\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.1163 - binary_accuracy: 0.9490 - val_loss: 0.1337 - val_binary_accuracy: 0.9436 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1114 - binary_accuracy: 0.9514\n",
      "Epoch 24: val_loss did not improve from 0.11531\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.1114 - binary_accuracy: 0.9514 - val_loss: 0.1700 - val_binary_accuracy: 0.9311 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1131 - binary_accuracy: 0.9505\n",
      "Epoch 25: val_loss improved from 0.11531 to 0.11447, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.1131 - binary_accuracy: 0.9505 - val_loss: 0.1145 - val_binary_accuracy: 0.9477 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1095 - binary_accuracy: 0.9519\n",
      "Epoch 26: val_loss improved from 0.11447 to 0.10766, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.1095 - binary_accuracy: 0.9519 - val_loss: 0.1077 - val_binary_accuracy: 0.9527 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1115 - binary_accuracy: 0.9514\n",
      "Epoch 27: val_loss did not improve from 0.10766\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.1115 - binary_accuracy: 0.9514 - val_loss: 0.1447 - val_binary_accuracy: 0.9380 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1088 - binary_accuracy: 0.9531\n",
      "Epoch 28: val_loss improved from 0.10766 to 0.10006, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.1088 - binary_accuracy: 0.9531 - val_loss: 0.1001 - val_binary_accuracy: 0.9565 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0978 - binary_accuracy: 0.9574\n",
      "Epoch 29: val_loss did not improve from 0.10006\n",
      "125/125 [==============================] - 25s 196ms/step - loss: 0.0978 - binary_accuracy: 0.9574 - val_loss: 0.1055 - val_binary_accuracy: 0.9547 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0947 - binary_accuracy: 0.9596\n",
      "Epoch 30: val_loss did not improve from 0.10006\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.0947 - binary_accuracy: 0.9596 - val_loss: 0.1055 - val_binary_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0914 - binary_accuracy: 0.9606\n",
      "Epoch 31: val_loss improved from 0.10006 to 0.09393, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.0914 - binary_accuracy: 0.9606 - val_loss: 0.0939 - val_binary_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0981 - binary_accuracy: 0.9577\n",
      "Epoch 32: val_loss did not improve from 0.09393\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.0981 - binary_accuracy: 0.9577 - val_loss: 0.1109 - val_binary_accuracy: 0.9523 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0928 - binary_accuracy: 0.9599\n",
      "Epoch 33: val_loss did not improve from 0.09393\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.0928 - binary_accuracy: 0.9599 - val_loss: 0.1079 - val_binary_accuracy: 0.9554 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0854 - binary_accuracy: 0.9627\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09393\n",
      "125/125 [==============================] - 24s 196ms/step - loss: 0.0854 - binary_accuracy: 0.9627 - val_loss: 0.1047 - val_binary_accuracy: 0.9553 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0744 - binary_accuracy: 0.9679\n",
      "Epoch 35: val_loss improved from 0.09393 to 0.08309, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.0744 - binary_accuracy: 0.9679 - val_loss: 0.0831 - val_binary_accuracy: 0.9640 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0702 - binary_accuracy: 0.9698\n",
      "Epoch 36: val_loss improved from 0.08309 to 0.07433, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.0702 - binary_accuracy: 0.9698 - val_loss: 0.0743 - val_binary_accuracy: 0.9677 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0671 - binary_accuracy: 0.9711\n",
      "Epoch 37: val_loss did not improve from 0.07433\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.0671 - binary_accuracy: 0.9711 - val_loss: 0.0758 - val_binary_accuracy: 0.9674 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0722 - binary_accuracy: 0.9691\n",
      "Epoch 38: val_loss did not improve from 0.07433\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0722 - binary_accuracy: 0.9691 - val_loss: 0.0816 - val_binary_accuracy: 0.9656 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0664 - binary_accuracy: 0.9714\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.07433\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0664 - binary_accuracy: 0.9714 - val_loss: 0.0942 - val_binary_accuracy: 0.9619 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0602 - binary_accuracy: 0.9742\n",
      "Epoch 40: val_loss improved from 0.07433 to 0.07199, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0602 - binary_accuracy: 0.9742 - val_loss: 0.0720 - val_binary_accuracy: 0.9696 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0605 - binary_accuracy: 0.9741\n",
      "Epoch 41: val_loss did not improve from 0.07199\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0605 - binary_accuracy: 0.9741 - val_loss: 0.0741 - val_binary_accuracy: 0.9695 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0579 - binary_accuracy: 0.9750\n",
      "Epoch 42: val_loss improved from 0.07199 to 0.07103, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0579 - binary_accuracy: 0.9750 - val_loss: 0.0710 - val_binary_accuracy: 0.9708 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0539 - binary_accuracy: 0.9768\n",
      "Epoch 43: val_loss improved from 0.07103 to 0.07095, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0539 - binary_accuracy: 0.9768 - val_loss: 0.0709 - val_binary_accuracy: 0.9708 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0577 - binary_accuracy: 0.9753\n",
      "Epoch 44: val_loss improved from 0.07095 to 0.06781, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0577 - binary_accuracy: 0.9753 - val_loss: 0.0678 - val_binary_accuracy: 0.9723 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0548 - binary_accuracy: 0.9765\n",
      "Epoch 45: val_loss did not improve from 0.06781\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0548 - binary_accuracy: 0.9765 - val_loss: 0.0865 - val_binary_accuracy: 0.9657 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0561 - binary_accuracy: 0.9758\n",
      "Epoch 46: val_loss did not improve from 0.06781\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0561 - binary_accuracy: 0.9758 - val_loss: 0.0698 - val_binary_accuracy: 0.9709 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0559 - binary_accuracy: 0.9761\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.06781\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0559 - binary_accuracy: 0.9761 - val_loss: 0.0786 - val_binary_accuracy: 0.9687 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0506 - binary_accuracy: 0.9781\n",
      "Epoch 48: val_loss did not improve from 0.06781\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0506 - binary_accuracy: 0.9781 - val_loss: 0.0697 - val_binary_accuracy: 0.9715 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0508 - binary_accuracy: 0.9782\n",
      "Epoch 49: val_loss did not improve from 0.06781\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0508 - binary_accuracy: 0.9782 - val_loss: 0.0701 - val_binary_accuracy: 0.9721 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0500 - binary_accuracy: 0.9787\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.06781\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0500 - binary_accuracy: 0.9787 - val_loss: 0.0686 - val_binary_accuracy: 0.9724 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "# === DeepRx: Train only (stable losses, no evaluation) ===\n",
    "\n",
    "# ---- Helpers ----\n",
    "def complex_to_2ch(x):\n",
    "    return tf.concat([tf.math.real(x), tf.math.imag(x)], axis=-1)\n",
    "\n",
    "def norm_per_sample(t):\n",
    "    # Normalize over (S,F,C) per sample for stability\n",
    "    mean = tf.reduce_mean(t, axis=[1,2,3], keepdims=True)\n",
    "    std  = tf.math.reduce_std(t, axis=[1,2,3], keepdims=True) + 1e-6\n",
    "    return (t - mean) / std\n",
    "\n",
    "# ---- ResNet config (adjust as you like) ----\n",
    "cfg = ResNetBlockProperties()\n",
    "cfg.num_blocks = 6\n",
    "cfg.kernel_size = [[3, 3]] * 6\n",
    "cfg.dilation_rate = [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4)]\n",
    "cfg.num_filter = [64, 64, 96, 96, 128, 128]\n",
    "\n",
    "# ---- Build model (your classes already defined above) ----\n",
    "deep_rx_model = DeepRx(\n",
    "    num_ofdm_sym=num_symbols,          # S\n",
    "    num_subcar=num_SC,                 # F\n",
    "    num_ant=1,                         # set to your N_r if >1 and adjust inputs accordingly\n",
    "    res_net_config=cfg,\n",
    "    num_output_llr=num_bits_per_symbol,# B\n",
    "    use_submodels=False\n",
    ").model\n",
    "\n",
    "deep_rx_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "# ---- Prepare inputs (NHWC) ----\n",
    "# y_data: (N, 1, S, F) complex64 -> (N, S, F, 2)\n",
    "y_in  = tf.squeeze(y_data, axis=1)\n",
    "y_in  = tf.expand_dims(y_in, -1)\n",
    "y_in  = complex_to_2ch(y_in)\n",
    "\n",
    "# h_data: ensure -> (N, S, F, 2)\n",
    "if len(h_data.shape) == 4 and h_data.shape[1] == 1:\n",
    "    h_core = tf.squeeze(h_data, axis=1)\n",
    "else:\n",
    "    h_core = h_data\n",
    "h_in  = complex_to_2ch(tf.expand_dims(h_core, -1))\n",
    "\n",
    "# pilots: zeros if no pilot grid available\n",
    "x_p_in = tf.zeros((tf.shape(y_in)[0], num_symbols, num_SC, 2), dtype=tf.float32)\n",
    "\n",
    "# Labels: HARD bits shaped to (N, S, F, B)\n",
    "llr_bits_full = tf.cast(llr > 0.0, tf.float32)\n",
    "llr_bits_full = tf.reshape(llr_bits_full, [-1, num_symbols, num_SC, num_bits_per_symbol])\n",
    "\n",
    "# ---- Train/Val split ----\n",
    "y_train   = tf.gather(y_in,  train_indices)\n",
    "y_val_in  = tf.gather(y_in,  val_indices)\n",
    "h_train   = tf.gather(h_in,  train_indices)\n",
    "h_val_in  = tf.gather(h_in,  val_indices)\n",
    "x_p_train = tf.gather(x_p_in, train_indices)\n",
    "x_p_val   = tf.gather(x_p_in, val_indices)\n",
    "lbl_train = tf.gather(llr_bits_full, train_indices)\n",
    "lbl_val   = tf.gather(llr_bits_full, val_indices)\n",
    "\n",
    "# ---- Optional: light normalization for stability (keep pilots as-is) ----\n",
    "y_train_n, y_val_n = norm_per_sample(y_train), norm_per_sample(y_val_in)\n",
    "h_train_n, h_val_n = norm_per_sample(h_train), norm_per_sample(h_val_in)\n",
    "x_p_train_n, x_p_val_n = x_p_train, x_p_val\n",
    "\n",
    "# ---- Sanity prints ----\n",
    "print(\"Inputs:\",\n",
    "      \"\\ny_train:\", y_train_n.shape,\n",
    "      \"\\nh_train:\", h_train_n.shape,\n",
    "      \"\\nx_p_train:\", x_p_train_n.shape,\n",
    "      \"\\nlabels:\", lbl_train.shape)\n",
    "\n",
    "# ---- Callbacks for stable training ----\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-5),\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('deeprx_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ---- Train (no evaluation here) ----\n",
    "history = deep_rx_model.fit(\n",
    "    [y_train_n, x_p_train_n, h_train_n], lbl_train,\n",
    "    validation_data=([y_val_n, x_p_val_n, h_val_n], lbl_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e316950-04e6-4122-87a2-df4a3be80635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "y_train: (4000, 14, 96, 2) \n",
      "h_train: (4000, 14, 96, 2) \n",
      "x_p_train: (4000, 14, 96, 2) \n",
      "labels (GT): (4000, 14, 96, 2)\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5448 - binary_accuracy: 0.6836\n",
      "Epoch 1: val_loss improved from inf to 0.68701, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 28s 202ms/step - loss: 0.5448 - binary_accuracy: 0.6836 - val_loss: 0.6870 - val_binary_accuracy: 0.5021 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4686 - binary_accuracy: 0.7578\n",
      "Epoch 2: val_loss improved from 0.68701 to 0.63529, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.4686 - binary_accuracy: 0.7578 - val_loss: 0.6353 - val_binary_accuracy: 0.5357 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4427 - binary_accuracy: 0.7731\n",
      "Epoch 3: val_loss improved from 0.63529 to 0.50072, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.4427 - binary_accuracy: 0.7731 - val_loss: 0.5007 - val_binary_accuracy: 0.6943 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4239 - binary_accuracy: 0.7814\n",
      "Epoch 4: val_loss improved from 0.50072 to 0.42999, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.4239 - binary_accuracy: 0.7814 - val_loss: 0.4300 - val_binary_accuracy: 0.7822 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4146 - binary_accuracy: 0.7874\n",
      "Epoch 5: val_loss improved from 0.42999 to 0.41701, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.4146 - binary_accuracy: 0.7874 - val_loss: 0.4170 - val_binary_accuracy: 0.7838 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4017 - binary_accuracy: 0.7937\n",
      "Epoch 6: val_loss improved from 0.41701 to 0.40241, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.4017 - binary_accuracy: 0.7937 - val_loss: 0.4024 - val_binary_accuracy: 0.7974 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3886 - binary_accuracy: 0.8020\n",
      "Epoch 7: val_loss improved from 0.40241 to 0.38465, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.3886 - binary_accuracy: 0.8020 - val_loss: 0.3847 - val_binary_accuracy: 0.8103 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3668 - binary_accuracy: 0.8172\n",
      "Epoch 8: val_loss did not improve from 0.38465\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3668 - binary_accuracy: 0.8172 - val_loss: 0.4003 - val_binary_accuracy: 0.8153 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3596 - binary_accuracy: 0.8209\n",
      "Epoch 9: val_loss improved from 0.38465 to 0.38266, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.3596 - binary_accuracy: 0.8209 - val_loss: 0.3827 - val_binary_accuracy: 0.8204 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3439 - binary_accuracy: 0.8293\n",
      "Epoch 10: val_loss improved from 0.38266 to 0.35655, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.3439 - binary_accuracy: 0.8293 - val_loss: 0.3566 - val_binary_accuracy: 0.8311 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3269 - binary_accuracy: 0.8387\n",
      "Epoch 11: val_loss improved from 0.35655 to 0.33861, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 198ms/step - loss: 0.3269 - binary_accuracy: 0.8387 - val_loss: 0.3386 - val_binary_accuracy: 0.8392 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3122 - binary_accuracy: 0.8453\n",
      "Epoch 12: val_loss improved from 0.33861 to 0.31580, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.3122 - binary_accuracy: 0.8453 - val_loss: 0.3158 - val_binary_accuracy: 0.8463 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2892 - binary_accuracy: 0.8580\n",
      "Epoch 13: val_loss did not improve from 0.31580\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.2892 - binary_accuracy: 0.8580 - val_loss: 0.3257 - val_binary_accuracy: 0.8520 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2642 - binary_accuracy: 0.8720\n",
      "Epoch 14: val_loss improved from 0.31580 to 0.29088, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.2642 - binary_accuracy: 0.8720 - val_loss: 0.2909 - val_binary_accuracy: 0.8649 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2465 - binary_accuracy: 0.8825\n",
      "Epoch 15: val_loss improved from 0.29088 to 0.25068, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.2465 - binary_accuracy: 0.8825 - val_loss: 0.2507 - val_binary_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2260 - binary_accuracy: 0.8925\n",
      "Epoch 16: val_loss improved from 0.25068 to 0.23562, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.2260 - binary_accuracy: 0.8925 - val_loss: 0.2356 - val_binary_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.2123 - binary_accuracy: 0.8997\n",
      "Epoch 17: val_loss did not improve from 0.23562\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.2123 - binary_accuracy: 0.8997 - val_loss: 0.2409 - val_binary_accuracy: 0.8877 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1949 - binary_accuracy: 0.9080\n",
      "Epoch 18: val_loss improved from 0.23562 to 0.20230, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1949 - binary_accuracy: 0.9080 - val_loss: 0.2023 - val_binary_accuracy: 0.9061 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1886 - binary_accuracy: 0.9102\n",
      "Epoch 19: val_loss improved from 0.20230 to 0.18845, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1886 - binary_accuracy: 0.9102 - val_loss: 0.1885 - val_binary_accuracy: 0.9118 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1794 - binary_accuracy: 0.9150\n",
      "Epoch 20: val_loss improved from 0.18845 to 0.18641, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1794 - binary_accuracy: 0.9150 - val_loss: 0.1864 - val_binary_accuracy: 0.9113 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1763 - binary_accuracy: 0.9163\n",
      "Epoch 21: val_loss did not improve from 0.18641\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1763 - binary_accuracy: 0.9163 - val_loss: 0.2092 - val_binary_accuracy: 0.9049 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1670 - binary_accuracy: 0.9209\n",
      "Epoch 22: val_loss improved from 0.18641 to 0.15302, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1670 - binary_accuracy: 0.9209 - val_loss: 0.1530 - val_binary_accuracy: 0.9279 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1574 - binary_accuracy: 0.9250\n",
      "Epoch 23: val_loss did not improve from 0.15302\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1574 - binary_accuracy: 0.9250 - val_loss: 0.1821 - val_binary_accuracy: 0.9151 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1527 - binary_accuracy: 0.9278\n",
      "Epoch 24: val_loss did not improve from 0.15302\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1527 - binary_accuracy: 0.9278 - val_loss: 0.2314 - val_binary_accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1549 - binary_accuracy: 0.9261\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.15302\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1549 - binary_accuracy: 0.9261 - val_loss: 0.1545 - val_binary_accuracy: 0.9284 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1312 - binary_accuracy: 0.9367\n",
      "Epoch 26: val_loss improved from 0.15302 to 0.13784, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1312 - binary_accuracy: 0.9367 - val_loss: 0.1378 - val_binary_accuracy: 0.9327 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1341 - binary_accuracy: 0.9356\n",
      "Epoch 27: val_loss did not improve from 0.13784\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.1341 - binary_accuracy: 0.9356 - val_loss: 0.1457 - val_binary_accuracy: 0.9286 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1293 - binary_accuracy: 0.9378\n",
      "Epoch 28: val_loss improved from 0.13784 to 0.13733, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1293 - binary_accuracy: 0.9378 - val_loss: 0.1373 - val_binary_accuracy: 0.9325 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1266 - binary_accuracy: 0.9389\n",
      "Epoch 29: val_loss did not improve from 0.13733\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1266 - binary_accuracy: 0.9389 - val_loss: 0.1397 - val_binary_accuracy: 0.9334 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1237 - binary_accuracy: 0.9402\n",
      "Epoch 30: val_loss improved from 0.13733 to 0.13611, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1237 - binary_accuracy: 0.9402 - val_loss: 0.1361 - val_binary_accuracy: 0.9352 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1225 - binary_accuracy: 0.9408\n",
      "Epoch 31: val_loss improved from 0.13611 to 0.13015, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1225 - binary_accuracy: 0.9408 - val_loss: 0.1301 - val_binary_accuracy: 0.9382 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1265 - binary_accuracy: 0.9389\n",
      "Epoch 32: val_loss did not improve from 0.13015\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1265 - binary_accuracy: 0.9389 - val_loss: 0.1370 - val_binary_accuracy: 0.9356 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1251 - binary_accuracy: 0.9399\n",
      "Epoch 33: val_loss did not improve from 0.13015\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1251 - binary_accuracy: 0.9399 - val_loss: 0.1730 - val_binary_accuracy: 0.9224 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1212 - binary_accuracy: 0.9410\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.13015\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1212 - binary_accuracy: 0.9410 - val_loss: 0.1376 - val_binary_accuracy: 0.9347 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1127 - binary_accuracy: 0.9449\n",
      "Epoch 35: val_loss improved from 0.13015 to 0.12368, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1127 - binary_accuracy: 0.9449 - val_loss: 0.1237 - val_binary_accuracy: 0.9396 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1127 - binary_accuracy: 0.9449\n",
      "Epoch 36: val_loss improved from 0.12368 to 0.12344, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1127 - binary_accuracy: 0.9449 - val_loss: 0.1234 - val_binary_accuracy: 0.9402 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1099 - binary_accuracy: 0.9463\n",
      "Epoch 37: val_loss did not improve from 0.12344\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.1099 - binary_accuracy: 0.9463 - val_loss: 0.1237 - val_binary_accuracy: 0.9410 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1092 - binary_accuracy: 0.9465\n",
      "Epoch 38: val_loss improved from 0.12344 to 0.12290, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1092 - binary_accuracy: 0.9465 - val_loss: 0.1229 - val_binary_accuracy: 0.9419 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1081 - binary_accuracy: 0.9469\n",
      "Epoch 39: val_loss did not improve from 0.12290\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1081 - binary_accuracy: 0.9469 - val_loss: 0.1275 - val_binary_accuracy: 0.9403 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1059 - binary_accuracy: 0.9478\n",
      "Epoch 40: val_loss improved from 0.12290 to 0.11990, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.1059 - binary_accuracy: 0.9478 - val_loss: 0.1199 - val_binary_accuracy: 0.9422 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1099 - binary_accuracy: 0.9463\n",
      "Epoch 41: val_loss did not improve from 0.11990\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1099 - binary_accuracy: 0.9463 - val_loss: 0.1258 - val_binary_accuracy: 0.9410 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1057 - binary_accuracy: 0.9478\n",
      "Epoch 42: val_loss did not improve from 0.11990\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1057 - binary_accuracy: 0.9478 - val_loss: 0.1294 - val_binary_accuracy: 0.9384 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1046 - binary_accuracy: 0.9483\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.11990\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1046 - binary_accuracy: 0.9483 - val_loss: 0.1250 - val_binary_accuracy: 0.9413 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1032 - binary_accuracy: 0.9489\n",
      "Epoch 44: val_loss improved from 0.11990 to 0.11982, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.1032 - binary_accuracy: 0.9489 - val_loss: 0.1198 - val_binary_accuracy: 0.9422 - lr: 1.2500e-04\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1017 - binary_accuracy: 0.9499\n",
      "Epoch 45: val_loss did not improve from 0.11982\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1017 - binary_accuracy: 0.9499 - val_loss: 0.1210 - val_binary_accuracy: 0.9432 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1023 - binary_accuracy: 0.9494\n",
      "Epoch 46: val_loss improved from 0.11982 to 0.11868, saving model to deeprx_best.h5\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.1023 - binary_accuracy: 0.9494 - val_loss: 0.1187 - val_binary_accuracy: 0.9433 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1020 - binary_accuracy: 0.9495\n",
      "Epoch 47: val_loss did not improve from 0.11868\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1020 - binary_accuracy: 0.9495 - val_loss: 0.1205 - val_binary_accuracy: 0.9430 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1001 - binary_accuracy: 0.9505\n",
      "Epoch 48: val_loss did not improve from 0.11868\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1001 - binary_accuracy: 0.9505 - val_loss: 0.1215 - val_binary_accuracy: 0.9425 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1001 - binary_accuracy: 0.9505\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.11868\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1001 - binary_accuracy: 0.9505 - val_loss: 0.1270 - val_binary_accuracy: 0.9403 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0988 - binary_accuracy: 0.9510\n",
      "Epoch 50: val_loss did not improve from 0.11868\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.0988 - binary_accuracy: 0.9510 - val_loss: 0.1193 - val_binary_accuracy: 0.9426 - lr: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "# === DeepRx: Train only (using GROUND-TRUTH bits as labels; no evaluation) ===\n",
    "\n",
    "# ---- Helpers ----\n",
    "def complex_to_2ch(x):\n",
    "    return tf.concat([tf.math.real(x), tf.math.imag(x)], axis=-1)\n",
    "\n",
    "def norm_per_sample(t):\n",
    "    # Normalize over (S,F,C) per sample for stability\n",
    "    mean = tf.reduce_mean(t, axis=[1,2,3], keepdims=True)\n",
    "    std  = tf.math.reduce_std(t, axis=[1,2,3], keepdims=True) + 1e-6\n",
    "    return (t - mean) / std\n",
    "\n",
    "# ---- ResNet config (adjust as you like) ----\n",
    "cfg = ResNetBlockProperties()\n",
    "cfg.num_blocks = 6\n",
    "cfg.kernel_size = [[3, 3]] * 6\n",
    "cfg.dilation_rate = [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (3, 4)]\n",
    "cfg.num_filter = [64, 64, 96, 96, 128, 128]\n",
    "\n",
    "# ---- Build model (your classes already defined above) ----\n",
    "deep_rx_model = DeepRx(\n",
    "    num_ofdm_sym=num_symbols,          # S\n",
    "    num_subcar=num_SC,                 # F\n",
    "    num_ant=1,                         # set to your N_r if >1 and adjust inputs accordingly\n",
    "    res_net_config=cfg,\n",
    "    num_output_llr=num_bits_per_symbol,# B\n",
    "    use_submodels=False\n",
    ").model\n",
    "\n",
    "deep_rx_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "# ---- Prepare inputs (NHWC) ----\n",
    "# y_data: (N, 1, S, F) complex64 -> (N, S, F, 2)\n",
    "y_in  = tf.squeeze(y_data, axis=1)\n",
    "y_in  = tf.expand_dims(y_in, -1)\n",
    "y_in  = complex_to_2ch(y_in)\n",
    "\n",
    "# h_data: ensure -> (N, S, F, 2)\n",
    "if len(h_data.shape) == 4 and h_data.shape[1] == 1:\n",
    "    h_core = tf.squeeze(h_data, axis=1)\n",
    "else:\n",
    "    h_core = h_data\n",
    "h_in  = complex_to_2ch(tf.expand_dims(h_core, -1))\n",
    "\n",
    "# pilots: zeros if no pilot grid available\n",
    "x_p_in = tf.zeros((tf.shape(y_in)[0], num_symbols, num_SC, 2), dtype=tf.float32)\n",
    "\n",
    "# ---- Labels: GROUND-TRUTH bits shaped to (N, S, F, B) ----\n",
    "# b: (N, num_ut, rg.num_streams_per_tx, S*F*B)\n",
    "b_all_flat = tf.squeeze(b, axis=[1, 2])  # -> (N, S*F*B)\n",
    "gt_bits = tf.reshape(tf.cast(b_all_flat, tf.float32),\n",
    "                     [-1, num_symbols, num_SC, num_bits_per_symbol])  # -> (N, S, F, B)\n",
    "\n",
    "# ---- Train/Val split ----\n",
    "y_train   = tf.gather(y_in,  train_indices)\n",
    "y_val_in  = tf.gather(y_in,  val_indices)\n",
    "h_train   = tf.gather(h_in,  train_indices)\n",
    "h_val_in  = tf.gather(h_in,  val_indices)\n",
    "x_p_train = tf.gather(x_p_in, train_indices)\n",
    "x_p_val   = tf.gather(x_p_in, val_indices)\n",
    "lbl_train = tf.gather(gt_bits, train_indices)   # (trainN, S, F, B)\n",
    "lbl_val   = tf.gather(gt_bits, val_indices)     # (valN,   S, F, B)\n",
    "\n",
    "# ---- Optional: light normalization for stability (keep pilots as-is) ----\n",
    "y_train_n, y_val_n = norm_per_sample(y_train), norm_per_sample(y_val_in)\n",
    "h_train_n, h_val_n = norm_per_sample(h_train), norm_per_sample(h_val_in)\n",
    "x_p_train_n, x_p_val_n = x_p_train, x_p_val\n",
    "\n",
    "# ---- Sanity prints ----\n",
    "print(\"Inputs:\",\n",
    "      \"\\ny_train:\", y_train_n.shape,\n",
    "      \"\\nh_train:\", h_train_n.shape,\n",
    "      \"\\nx_p_train:\", x_p_train_n.shape,\n",
    "      \"\\nlabels (GT):\", lbl_train.shape)\n",
    "\n",
    "# ---- Callbacks for stable training ----\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-5),\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('deeprx_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ---- Train (no evaluation here) ----\n",
    "history = deep_rx_model.fit(\n",
    "    [y_train_n, x_p_train_n, h_train_n], lbl_train,\n",
    "    validation_data=([y_val_n, x_p_val_n, h_val_n], lbl_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c8ebe7a-5977-425d-bf25-3bb5d554d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best weights from 'deeprx_best.h5'.\n",
      "Neural Rx (DeepRx) BER [validation]: 0.054094\n",
      "Classical ZF BER [validation]: 0.035301\n",
      "Relative BER change (Neural vs ZF): +53.24%\n"
     ]
    }
   ],
   "source": [
    "# === BER Benchmark: Neural Rx (DeepRx) vs Classical/ZF on validation split ===\n",
    "\n",
    "import os\n",
    "\n",
    "# 1) (Optional) load best checkpoint before evaluating\n",
    "try:\n",
    "    if os.path.exists('deeprx_best.h5'):\n",
    "        deep_rx_model.load_weights('deeprx_best.h5')\n",
    "        print(\"Loaded best weights from 'deeprx_best.h5'.\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping weight load:\", e)\n",
    "\n",
    "# 2) Predict LLR logits with DeepRx and hard-decode to bits\n",
    "llr_pred_neural = deep_rx_model.predict([y_val_n, x_p_val_n, h_val_n], batch_size=32, verbose=0)  # (valN, S, F, B)\n",
    "bits_neural = tf.cast(llr_pred_neural > 0.0, tf.float32)                                         # (valN, S, F, B)\n",
    "bits_neural_flat = tf.reshape(bits_neural, [tf.shape(bits_neural)[0], -1])                        # (valN, S*F*B)\n",
    "\n",
    "# 3) Ground-truth bits matching the same validation subset\n",
    "#    b: (N, num_ut, rg.num_streams_per_tx, S*F*B) from your Tx pipeline\n",
    "b_all_flat = tf.squeeze(b, axis=[1, 2])                                   # (N, S*F*B)\n",
    "b_val_flat = tf.cast(tf.gather(b_all_flat, val_indices), tf.float32)      # (valN, S*F*B)\n",
    "\n",
    "# Sanity check: shapes must match for BER\n",
    "assert int(b_val_flat.shape[1]) == int(bits_neural_flat.shape[1]), \\\n",
    "    f\"GT bits and Neural bits mismatch: {b_val_flat.shape[1]} vs {bits_neural_flat.shape[1]}\"\n",
    "\n",
    "# 4) Neural Rx BER\n",
    "ber_neural = compute_ber(b_val_flat, bits_neural_flat).numpy()\n",
    "print(f\"Neural Rx (DeepRx) BER [validation]: {ber_neural:.6f}\")\n",
    "\n",
    "# 5) Classical/ZF baseline BER on the SAME validation split\n",
    "#    llr: (N, S, F, B) from your earlier ZF demapper (before training)\n",
    "try:\n",
    "    bits_zf = tf.cast(llr > 0.0, tf.float32)                               # (N, S, F, B)\n",
    "    bits_zf_flat = tf.reshape(bits_zf, [tf.shape(bits_zf)[0], -1])         # (N, S*F*B)\n",
    "    bits_zf_val_flat = tf.gather(bits_zf_flat, val_indices)                # (valN, S*F*B)\n",
    "\n",
    "    # Sanity check\n",
    "    assert int(b_val_flat.shape[1]) == int(bits_zf_val_flat.shape[1]), \\\n",
    "        f\"GT bits and ZF bits mismatch: {b_val_flat.shape[1]} vs {bits_zf_val_flat.shape[1]}\"\n",
    "\n",
    "    ber_zf = compute_ber(b_val_flat, bits_zf_val_flat).numpy()\n",
    "    print(f\"Classical ZF BER [validation]: {ber_zf:.6f}\")\n",
    "\n",
    "    # 6) Relative comparison\n",
    "    if ber_zf > 0:\n",
    "        rel = (ber_neural - ber_zf) / ber_zf\n",
    "        print(f\"Relative BER change (Neural vs ZF): {rel:+.2%}\")\n",
    "except Exception as e:\n",
    "    print(\"ZF baseline BER skipped (missing or incompatible 'llr'):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be593b-803f-43d9-ab72-b35bec90ff55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
